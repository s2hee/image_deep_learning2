{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. GPU Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/s2hee/opt/anaconda3/envs/sehee/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(5,5)\n",
    " \n",
    "    def forward(self,x):\n",
    "        net = self.lin1(x)\n",
    "        return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.7489, -0.7436,  0.6373, -0.5621, -0.1569], device='mps:0',\n",
      "       grad_fn=<LinearBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/s2hee/opt/anaconda3/envs/sehee/lib/python3.9/site-packages/torch/_tensor_str.py:115: UserWarning: The operator 'aten::nonzero' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1667808783679/work/aten/src/ATen/mps/MPSFallback.mm:11.)\n",
      "  nonzero_finite_vals = torch.masked_select(\n"
     ]
    }
   ],
   "source": [
    "# MPS 장치에 바로 tensor를 생성합니다.\n",
    "x = torch.ones(5, device=device)\n",
    " \n",
    "# GPU 상에서 연산 진행\n",
    "y = x * 2\n",
    " \n",
    "# 또는, 다른 장치와 마찬가지로 MPS로 이동할 수도 있습니다.\n",
    "model = Net()# 어떤 모델의 객체를 생성한 뒤,\n",
    "model.to(device) # MPS 장치로 이동합니다.\n",
    " \n",
    "# 이제 모델과 텐서를 호출하면 GPU에서 연산이 이뤄집니다.\n",
    "pred = model(x)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version:1.14.0.dev20221107\n",
      "MPS 장치를 지원하도록 build 되었는지: True\n",
      "MPS 장치가 사용 가능한지: True\n",
      "macOS-10.16-x86_64-i386-64bit\n"
     ]
    }
   ],
   "source": [
    "print (f\"PyTorch version:{torch.__version__}\") # 1.12.1 이상\n",
    "print(f\"MPS 장치를 지원하도록 build 되었는지: {torch.backends.mps.is_built()}\") # True 여야 합니다.\n",
    "print(f\"MPS 장치가 사용 가능한지: {torch.backends.mps.is_available()}\") # True 여야 합니다.\n",
    "!python -c 'import platform;print(platform.platform())'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading annotations.json\n",
    "TRAIN_ANNOTATIONS_PATH = \"/Users/s2hee/Desktop/deep_learning/public_training_set_release_2.1/training_annotations.json\"\n",
    "TRAIN_IMAGE_DIRECTIORY = \"/Users/s2hee/Desktop/deep_learning/public_training_set_release_2.1/images/\"\n",
    "\n",
    "VAL_ANNOTATIONS_PATH = \"/Users/s2hee/Desktop/deep_learning/public_validation_set_release_2.1/vdset_annotations.json\"\n",
    "VAL_IMAGE_DIRECTIORY = \"/Users/s2hee/Desktop/deep_learning/public_validation_set_release_2.1/images/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=2.68s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# For reading annotations file\n",
    "import pycocotools\n",
    "from pycocotools.coco import COCO\n",
    "train_coco = COCO(TRAIN_ANNOTATIONS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the annotation files\n",
    "import json\n",
    "with open(TRAIN_ANNOTATIONS_PATH) as f:\n",
    "  train_annotations_data = json.load(f)\n",
    "\n",
    "with open(VAL_ANNOTATIONS_PATH) as f:\n",
    "  val_annotations_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 184123,\n",
       " 'image_id': 131072,\n",
       " 'category_id': 101246,\n",
       " 'segmentation': [[169.0,\n",
       "   379.5,\n",
       "   130.0,\n",
       "   374.5,\n",
       "   112.0,\n",
       "   363.5,\n",
       "   94.5,\n",
       "   340.0,\n",
       "   61.5,\n",
       "   213.0,\n",
       "   61.5,\n",
       "   188.0,\n",
       "   70.5,\n",
       "   168.0,\n",
       "   87.0,\n",
       "   152.5,\n",
       "   103.0,\n",
       "   143.5,\n",
       "   123.0,\n",
       "   139.5,\n",
       "   185.0,\n",
       "   118.5,\n",
       "   226.0,\n",
       "   90.5,\n",
       "   249.0,\n",
       "   87.5,\n",
       "   309.0,\n",
       "   88.5,\n",
       "   339.0,\n",
       "   110.5,\n",
       "   350.5,\n",
       "   125.00000000000001,\n",
       "   354.5,\n",
       "   155.0,\n",
       "   382.5,\n",
       "   231.0,\n",
       "   383.5,\n",
       "   277.0,\n",
       "   360.0,\n",
       "   303.5,\n",
       "   327.0,\n",
       "   331.5,\n",
       "   308.0,\n",
       "   343.5,\n",
       "   216.0,\n",
       "   373.5]],\n",
       " 'area': 71393.0,\n",
       " 'bbox': [61.5, 61.5, 318.0, 322.0],\n",
       " 'iscrowd': 0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_annotations_data['annotations'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 233459,\n",
       " 'image_id': 149022,\n",
       " 'category_id': 101182,\n",
       " 'segmentation': [[214.0,\n",
       "   152.5,\n",
       "   175.0,\n",
       "   144.5,\n",
       "   156.5,\n",
       "   133.0,\n",
       "   153.5,\n",
       "   119.0,\n",
       "   159.5,\n",
       "   99.0,\n",
       "   168.0,\n",
       "   89.5,\n",
       "   191.0,\n",
       "   77.5,\n",
       "   231.99999999999997,\n",
       "   73.5,\n",
       "   258.0,\n",
       "   80.5,\n",
       "   279.5,\n",
       "   95.0,\n",
       "   285.5,\n",
       "   104.0,\n",
       "   288.5,\n",
       "   122.0,\n",
       "   278.0,\n",
       "   137.5,\n",
       "   250.00000000000003,\n",
       "   148.5]],\n",
       " 'area': 8225.0,\n",
       " 'bbox': [77.5, 153.5, 136.5, 135.0],\n",
       " 'iscrowd': 0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_annotations_data['annotations'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Categories\n",
      "- Beetroot, steamed, without addition of salt\n",
      "- bread_wholemeal\n",
      "- jam\n",
      "- water\n",
      "- bread\n",
      "- banana\n",
      "- soft_cheese\n",
      "- ham_raw\n",
      "- hard_cheese\n",
      "- cottage_cheese\n",
      "- coffee\n",
      "- fruit_mixed\n",
      "- pancake\n",
      "- tea\n",
      "- salmon_smoked\n",
      "- avocado\n",
      "- spring_onion_scallion\n",
      "- ristretto_with_caffeine\n",
      "- ham_n_s\n",
      "- egg\n",
      "- bacon\n",
      "- chips_french_fries\n",
      "- juice_apple\n",
      "- chicken\n",
      "- tomato\n",
      "- broccoli\n",
      "- shrimp_prawn\n",
      "- carrot\n",
      "- chickpeas\n",
      "- french_salad_dressing\n",
      "- pasta_hornli_ch\n",
      "- sauce_cream\n",
      "- pasta_n_s\n",
      "- tomato_sauce\n",
      "- cheese_n_s\n",
      "- pear\n",
      "- cashew_nut\n",
      "- almonds\n",
      "- lentil_n_s\n",
      "- mixed_vegetables\n",
      "- peanut_butter\n",
      "- apple\n",
      "- blueberries\n",
      "- cucumber\n",
      "- yogurt\n",
      "- butter\n",
      "- mayonnaise\n",
      "- soup\n",
      "- wine_red\n",
      "- wine_white\n",
      "- green_bean_steamed_without_addition_of_salt\n",
      "- sausage\n",
      "- pizza_margherita_baked\n",
      "- salami_ch\n",
      "- mushroom\n",
      "- tart_n_s\n",
      "- rice\n",
      "- white_coffee\n",
      "- sunflower_seeds\n",
      "- bell_pepper_red_raw\n",
      "- zucchini\n",
      "- asparagus\n",
      "- tartar_sauce\n",
      "- lye_pretzel_soft\n",
      "- cucumber_pickled_ch\n",
      "- curry_vegetarian\n",
      "- soup_of_lentils_dahl_dhal\n",
      "- salmon\n",
      "- salt_cake_ch_vegetables_filled\n",
      "- orange\n",
      "- pasta_noodles\n",
      "- cream_double_cream_heavy_cream_45\n",
      "- cake_chocolate\n",
      "- pasta_spaghetti\n",
      "- black_olives\n",
      "- parmesan\n",
      "- spaetzle\n",
      "- salad_lambs_ear\n",
      "- salad_leaf_salad_green\n",
      "- potato\n",
      "- white_cabbage\n",
      "- halloumi\n",
      "- beetroot_raw\n",
      "- bread_grain\n",
      "- applesauce\n",
      "- cheese_for_raclette_ch\n",
      "- bread_white\n",
      "- curds_natural\n",
      "- quiche\n",
      "- beef_n_s\n",
      "- taboule_prepared_with_couscous\n",
      "- aubergine_eggplant\n",
      "- mozzarella\n",
      "- pasta_penne\n",
      "- lasagne_vegetable_prepared\n",
      "- mandarine\n",
      "- kiwi\n",
      "- french_beans\n",
      "- spring_roll_fried\n",
      "- caprese_salad_tomato_mozzarella\n",
      "- leaf_spinach\n",
      "- roll_of_half_white_or_white_flour_with_large_void\n",
      "- omelette_with_flour_thick_crepe_plain\n",
      "- tuna\n",
      "- dark_chocolate\n",
      "- sauce_savoury_n_s\n",
      "- raisins_dried\n",
      "- ice_tea_on_black_tea_basis\n",
      "- kaki\n",
      "- smoothie\n",
      "- crepe_with_flour_plain\n",
      "- nuggets\n",
      "- chili_con_carne_prepared\n",
      "- veggie_burger\n",
      "- chinese_cabbage\n",
      "- hamburger\n",
      "- soup_pumpkin\n",
      "- sushi\n",
      "- chestnuts_ch\n",
      "- sauce_soya\n",
      "- balsamic_salad_dressing\n",
      "- pasta_twist\n",
      "- bolognaise_sauce\n",
      "- leek\n",
      "- fajita_bread_only\n",
      "- potato_gnocchi\n",
      "- rice_noodles_vermicelli\n",
      "- bread_whole_wheat\n",
      "- onion\n",
      "- garlic\n",
      "- hummus\n",
      "- pizza_with_vegetables_baked\n",
      "- beer\n",
      "- glucose_drink_50g\n",
      "- ratatouille\n",
      "- peanut\n",
      "- cauliflower\n",
      "- green_olives\n",
      "- bread_pita\n",
      "- pasta_wholemeal\n",
      "- sauce_pesto\n",
      "- couscous\n",
      "- sauce\n",
      "- bread_toast\n",
      "- water_with_lemon_juice\n",
      "- espresso\n",
      "- egg_scrambled\n",
      "- juice_orange\n",
      "- braided_white_loaf_ch\n",
      "- emmental_cheese_ch\n",
      "- hazelnut_chocolate_spread_nutella_ovomaltine_caotina\n",
      "- tomme_ch\n",
      "- hazelnut\n",
      "- peach\n",
      "- figs\n",
      "- mashed_potatoes_prepared_with_full_fat_milk_with_butter\n",
      "- pumpkin\n",
      "- swiss_chard\n",
      "- red_cabbage_raw\n",
      "- spinach_raw\n",
      "- chicken_curry_cream_coconut_milk_curry_spices_paste\n",
      "- crunch_muesli\n",
      "- biscuit\n",
      "- meatloaf_ch\n",
      "- fresh_cheese_n_s\n",
      "- honey\n",
      "- vegetable_mix_peas_and_carrots\n",
      "- parsley\n",
      "- brownie\n",
      "- ice_cream_n_s\n",
      "- salad_dressing\n",
      "- dried_meat_n_s\n",
      "- chicken_breast\n",
      "- mixed_salad_chopped_without_sauce\n",
      "- feta\n",
      "- praline_n_s\n",
      "- walnut\n",
      "- potato_salad\n",
      "- kolhrabi\n",
      "- alfa_sprouts\n",
      "- brussel_sprouts\n",
      "- gruyere_ch\n",
      "- bulgur\n",
      "- grapes\n",
      "- chocolate_egg_small\n",
      "- cappuccino\n",
      "- crisp_bread\n",
      "- bread_black\n",
      "- rosti_n_s\n",
      "- mango\n",
      "- muesli_dry\n",
      "- spinach\n",
      "- fish_n_s\n",
      "- risotto\n",
      "- crisps_ch\n",
      "- pork_n_s\n",
      "- pomegranate\n",
      "- sweet_corn\n",
      "- flakes\n",
      "- greek_salad\n",
      "- sesame_seeds\n",
      "- bouillon\n",
      "- baked_potato\n",
      "- fennel\n",
      "- meat_n_s\n",
      "- croutons\n",
      "- bell_pepper_red_stewed\n",
      "- nuts\n",
      "- breadcrumbs_unspiced\n",
      "- fondue\n",
      "- sauce_mushroom\n",
      "- strawberries\n",
      "- pie_plum_baked_with_cake_dough\n",
      "- potatoes_au_gratin_dauphinois_prepared\n",
      "- capers\n",
      "- bread_wholemeal_toast\n",
      "- red_radish\n",
      "- fruit_tart\n",
      "- beans_kidney\n",
      "- sauerkraut\n",
      "- mustard\n",
      "- country_fries\n",
      "- ketchup\n",
      "- pasta_linguini_parpadelle_tagliatelle\n",
      "- chicken_cut_into_stripes_only_meat\n",
      "- cookies\n",
      "- sun_dried_tomatoe\n",
      "- bread_ticino_ch\n",
      "- semi_hard_cheese\n",
      "- porridge_prepared_with_partially_skimmed_milk\n",
      "- juice\n",
      "- chocolate_milk\n",
      "- bread_fruit\n",
      "- corn\n",
      "- dates\n",
      "- pistachio\n",
      "- cream_cheese_n_s\n",
      "- bread_rye\n",
      "- witloof_chicory\n",
      "- goat_cheese_soft\n",
      "- grapefruit_pomelo\n",
      "- blue_mould_cheese\n",
      "- guacamole\n",
      "- tofu\n",
      "- cordon_bleu\n",
      "- quinoa\n",
      "- kefir_drink\n",
      "- salad_rocket\n",
      "- pizza_with_ham_with_mushrooms_baked\n",
      "- fruit_coulis\n",
      "- plums\n",
      "- pizza_with_ham_baked\n",
      "- pineapple\n",
      "- seeds_n_s\n",
      "- focaccia\n",
      "- mixed_milk_beverage\n",
      "- coleslaw_chopped_without_sauce\n",
      "- sweet_potato\n",
      "- chicken_leg\n",
      "- croissant\n",
      "- cheesecake\n",
      "- sauce_cocktail\n",
      "- croissant_with_chocolate_filling\n",
      "- pumpkin_seeds\n",
      "- artichoke\n",
      "- soft_drink_with_a_taste\n",
      "- apple_pie\n",
      "- white_bread_with_butter_eggs_and_milk\n",
      "- savoury_pastry_stick\n",
      "- tuna_in_oil_drained\n",
      "- meat_terrine_pate\n",
      "- falafel_balls\n",
      "- berries_n_s\n",
      "- latte_macchiato\n",
      "- sugar_melon_galia_honeydew_cantaloupe\n",
      "- mixed_seeds_n_s\n",
      "- oil_vinegar_salad_dressing\n",
      "- celeriac\n",
      "- chocolate_mousse\n",
      "- lemon\n",
      "- chocolate_cookies\n",
      "- birchermuesli_prepared_no_sugar_added\n",
      "- muffin\n",
      "- pine_nuts\n",
      "- french_pizza_from_alsace_baked\n",
      "- chocolate_n_s\n",
      "- grits_polenta_maize_flour\n",
      "- wine_rose\n",
      "- cola_based_drink\n",
      "- raspberries\n",
      "- roll_with_pieces_of_chocolate\n",
      "- cake_lemon\n",
      "- rice_wild\n",
      "- gluten_free_bread\n",
      "- pearl_onion\n",
      "- tzatziki\n",
      "- ham_croissant_ch\n",
      "- corn_crisps\n",
      "- lentils_green_du_puy_du_berry\n",
      "- rice_whole_grain\n",
      "- cervelat_ch\n",
      "- aperitif_with_alcohol_n_s_aperol_spritz\n",
      "- peas\n",
      "- tiramisu\n",
      "- apricots\n",
      "- lasagne_meat_prepared\n",
      "- brioche\n",
      "- vegetable_au_gratin_baked\n",
      "- basil\n",
      "- butter_spread_puree_almond\n",
      "- pie_apricot\n",
      "- rusk_wholemeal\n",
      "- pasta_in_conch_form\n",
      "- pasta_in_butterfly_form_farfalle\n",
      "- damson_plum\n",
      "- shoots_n_s\n",
      "- coconut\n",
      "- banana_cake\n",
      "- sauce_curry\n",
      "- watermelon_fresh\n",
      "- white_asparagus\n",
      "- cherries\n",
      "- nectarine\n"
     ]
    }
   ],
   "source": [
    "# Reading all classes\n",
    "category_ids = train_coco.loadCats(train_coco.getCatIds())\n",
    "category_names = [_[\"name_readable\"] for _ in category_ids]\n",
    "\n",
    "print(\"## Categories\\n-\", \"\\n- \".join(category_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           id   file_name  width  height\n",
      "0      131072  131072.jpg    464     464\n",
      "1      131087  131087.jpg    464     464\n",
      "2      131088  131088.jpg    511     512\n",
      "3      131094  131094.jpg    480     480\n",
      "4      131096  131096.jpg    464     464\n",
      "...       ...         ...    ...     ...\n",
      "54387  131024  131024.jpg    512     910\n",
      "54388  131033  131033.jpg    455     455\n",
      "54389  131053  131053.jpg    391     390\n",
      "54390  131066  131066.jpg    464     464\n",
      "54391  131071  131071.jpg    464     464\n",
      "\n",
      "[54392 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "img_info = pd.DataFrame(train_coco.loadImgs(train_coco.getImgIds()))\n",
    "print(img_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/s2hee/Desktop/deep_learning/basic_U_Net_.ipynb 셀 15\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/s2hee/Desktop/deep_learning/basic_U_Net_.ipynb#X20sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     plt\u001b[39m.\u001b[39mimshow(mask)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/s2hee/Desktop/deep_learning/basic_U_Net_.ipynb#X20sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     plt\u001b[39m.\u001b[39maxis(\u001b[39m\"\u001b[39m\u001b[39moff\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/s2hee/Desktop/deep_learning/basic_U_Net_.ipynb#X20sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     plt\u001b[39m.\u001b[39;49msavefig(fname\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmasks/\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39m+\u001b[39;49m \u001b[39mstr\u001b[39;49m(train_annotations_data[\u001b[39m'\u001b[39;49m\u001b[39mimages\u001b[39;49m\u001b[39m'\u001b[39;49m][img_no][\u001b[39m'\u001b[39;49m\u001b[39mid\u001b[39;49m\u001b[39m'\u001b[39;49m]) \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m.png\u001b[39;49m\u001b[39m'\u001b[39;49m, bbox_inches\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtight\u001b[39;49m\u001b[39m'\u001b[39;49m, pad_inches\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/s2hee/Desktop/deep_learning/basic_U_Net_.ipynb#X20sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# plot 초기화\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/s2hee/Desktop/deep_learning/basic_U_Net_.ipynb#X20sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m plt\u001b[39m.\u001b[39mclf()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sehee/lib/python3.9/site-packages/matplotlib/pyplot.py:978\u001b[0m, in \u001b[0;36msavefig\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    976\u001b[0m fig \u001b[39m=\u001b[39m gcf()\n\u001b[1;32m    977\u001b[0m res \u001b[39m=\u001b[39m fig\u001b[39m.\u001b[39msavefig(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 978\u001b[0m fig\u001b[39m.\u001b[39;49mcanvas\u001b[39m.\u001b[39;49mdraw_idle()   \u001b[39m# need this if 'transparent=True' to reset colors\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[39mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sehee/lib/python3.9/site-packages/matplotlib/backend_bases.py:2060\u001b[0m, in \u001b[0;36mFigureCanvasBase.draw_idle\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2058\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_idle_drawing:\n\u001b[1;32m   2059\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_idle_draw_cntx():\n\u001b[0;32m-> 2060\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdraw(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sehee/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:436\u001b[0m, in \u001b[0;36mFigureCanvasAgg.draw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[39m# Acquire a lock on the shared font cache.\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[39mwith\u001b[39;00m RendererAgg\u001b[39m.\u001b[39mlock, \\\n\u001b[1;32m    434\u001b[0m      (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoolbar\u001b[39m.\u001b[39m_wait_cursor_for_draw_cm() \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoolbar\n\u001b[1;32m    435\u001b[0m       \u001b[39melse\u001b[39;00m nullcontext()):\n\u001b[0;32m--> 436\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfigure\u001b[39m.\u001b[39;49mdraw(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrenderer)\n\u001b[1;32m    437\u001b[0m     \u001b[39m# A GUI class may be need to update a window using this draw, so\u001b[39;00m\n\u001b[1;32m    438\u001b[0m     \u001b[39m# don't forget to call the superclass.\u001b[39;00m\n\u001b[1;32m    439\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mdraw()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sehee/lib/python3.9/site-packages/matplotlib/artist.py:74\u001b[0m, in \u001b[0;36m_finalize_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39m@wraps\u001b[39m(draw)\n\u001b[1;32m     73\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdraw_wrapper\u001b[39m(artist, renderer, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 74\u001b[0m     result \u001b[39m=\u001b[39m draw(artist, renderer, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     75\u001b[0m     \u001b[39mif\u001b[39;00m renderer\u001b[39m.\u001b[39m_rasterizing:\n\u001b[1;32m     76\u001b[0m         renderer\u001b[39m.\u001b[39mstop_rasterizing()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sehee/lib/python3.9/site-packages/matplotlib/artist.py:51\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m         renderer\u001b[39m.\u001b[39mstart_filter()\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m draw(artist, renderer)\n\u001b[1;32m     52\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sehee/lib/python3.9/site-packages/matplotlib/figure.py:2845\u001b[0m, in \u001b[0;36mFigure.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   2842\u001b[0m         \u001b[39m# ValueError can occur when resizing a window.\u001b[39;00m\n\u001b[1;32m   2844\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpatch\u001b[39m.\u001b[39mdraw(renderer)\n\u001b[0;32m-> 2845\u001b[0m mimage\u001b[39m.\u001b[39;49m_draw_list_compositing_images(\n\u001b[1;32m   2846\u001b[0m     renderer, \u001b[39mself\u001b[39;49m, artists, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msuppressComposite)\n\u001b[1;32m   2848\u001b[0m \u001b[39mfor\u001b[39;00m sfig \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubfigs:\n\u001b[1;32m   2849\u001b[0m     sfig\u001b[39m.\u001b[39mdraw(renderer)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sehee/lib/python3.9/site-packages/matplotlib/image.py:132\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[39mif\u001b[39;00m not_composite \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m has_images:\n\u001b[1;32m    131\u001b[0m     \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m artists:\n\u001b[0;32m--> 132\u001b[0m         a\u001b[39m.\u001b[39;49mdraw(renderer)\n\u001b[1;32m    133\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m     \u001b[39m# Composite any adjacent images together\u001b[39;00m\n\u001b[1;32m    135\u001b[0m     image_group \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sehee/lib/python3.9/site-packages/matplotlib/artist.py:51\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m         renderer\u001b[39m.\u001b[39mstart_filter()\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m draw(artist, renderer)\n\u001b[1;32m     52\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sehee/lib/python3.9/site-packages/matplotlib/axes/_base.py:3091\u001b[0m, in \u001b[0;36m_AxesBase.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3088\u001b[0m         a\u001b[39m.\u001b[39mdraw(renderer)\n\u001b[1;32m   3089\u001b[0m     renderer\u001b[39m.\u001b[39mstop_rasterizing()\n\u001b[0;32m-> 3091\u001b[0m mimage\u001b[39m.\u001b[39;49m_draw_list_compositing_images(\n\u001b[1;32m   3092\u001b[0m     renderer, \u001b[39mself\u001b[39;49m, artists, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfigure\u001b[39m.\u001b[39;49msuppressComposite)\n\u001b[1;32m   3094\u001b[0m renderer\u001b[39m.\u001b[39mclose_group(\u001b[39m'\u001b[39m\u001b[39maxes\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m   3095\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstale \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sehee/lib/python3.9/site-packages/matplotlib/image.py:132\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[39mif\u001b[39;00m not_composite \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m has_images:\n\u001b[1;32m    131\u001b[0m     \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m artists:\n\u001b[0;32m--> 132\u001b[0m         a\u001b[39m.\u001b[39;49mdraw(renderer)\n\u001b[1;32m    133\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m     \u001b[39m# Composite any adjacent images together\u001b[39;00m\n\u001b[1;32m    135\u001b[0m     image_group \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sehee/lib/python3.9/site-packages/matplotlib/artist.py:51\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m         renderer\u001b[39m.\u001b[39mstart_filter()\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m draw(artist, renderer)\n\u001b[1;32m     52\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sehee/lib/python3.9/site-packages/matplotlib/image.py:646\u001b[0m, in \u001b[0;36m_ImageBase.draw\u001b[0;34m(self, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    644\u001b[0m         renderer\u001b[39m.\u001b[39mdraw_image(gc, l, b, im, trans)\n\u001b[1;32m    645\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 646\u001b[0m     im, l, b, trans \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmake_image(\n\u001b[1;32m    647\u001b[0m         renderer, renderer\u001b[39m.\u001b[39;49mget_image_magnification())\n\u001b[1;32m    648\u001b[0m     \u001b[39mif\u001b[39;00m im \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    649\u001b[0m         renderer\u001b[39m.\u001b[39mdraw_image(gc, l, b, im)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sehee/lib/python3.9/site-packages/matplotlib/image.py:956\u001b[0m, in \u001b[0;36mAxesImage.make_image\u001b[0;34m(self, renderer, magnification, unsampled)\u001b[0m\n\u001b[1;32m    953\u001b[0m transformed_bbox \u001b[39m=\u001b[39m TransformedBbox(bbox, trans)\n\u001b[1;32m    954\u001b[0m clip \u001b[39m=\u001b[39m ((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_clip_box() \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes\u001b[39m.\u001b[39mbbox) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_clip_on()\n\u001b[1;32m    955\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfigure\u001b[39m.\u001b[39mbbox)\n\u001b[0;32m--> 956\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_image(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_A, bbox, transformed_bbox, clip,\n\u001b[1;32m    957\u001b[0m                         magnification, unsampled\u001b[39m=\u001b[39;49munsampled)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sehee/lib/python3.9/site-packages/matplotlib/image.py:557\u001b[0m, in \u001b[0;36m_ImageBase._make_image\u001b[0;34m(self, A, in_bbox, out_bbox, clip_bbox, magnification, unsampled, round_to_pixel_border)\u001b[0m\n\u001b[1;32m    554\u001b[0m     alpha \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_scalar_alpha()\n\u001b[1;32m    555\u001b[0m     output_alpha \u001b[39m=\u001b[39m _resample(  \u001b[39m# resample alpha channel\u001b[39;00m\n\u001b[1;32m    556\u001b[0m         \u001b[39mself\u001b[39m, A[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39m3\u001b[39m], out_shape, t, alpha\u001b[39m=\u001b[39malpha)\n\u001b[0;32m--> 557\u001b[0m     output \u001b[39m=\u001b[39m _resample(  \u001b[39m# resample rgb channels\u001b[39;49;00m\n\u001b[1;32m    558\u001b[0m         \u001b[39mself\u001b[39;49m, _rgb_to_rgba(A[\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m, :\u001b[39m3\u001b[39;49m]), out_shape, t, alpha\u001b[39m=\u001b[39;49malpha)\n\u001b[1;32m    559\u001b[0m     output[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39m3\u001b[39m] \u001b[39m=\u001b[39m output_alpha  \u001b[39m# recombine rgb and alpha\u001b[39;00m\n\u001b[1;32m    561\u001b[0m \u001b[39m# at this point output is either a 2D array of normed data\u001b[39;00m\n\u001b[1;32m    562\u001b[0m \u001b[39m# (of int or float)\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[39m# or an RGBA array of re-sampled input\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sehee/lib/python3.9/site-packages/matplotlib/image.py:193\u001b[0m, in \u001b[0;36m_resample\u001b[0;34m(image_obj, data, out_shape, transform, resample, alpha)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[39mif\u001b[39;00m resample \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    192\u001b[0m     resample \u001b[39m=\u001b[39m image_obj\u001b[39m.\u001b[39mget_resample()\n\u001b[0;32m--> 193\u001b[0m _image\u001b[39m.\u001b[39;49mresample(data, out, transform,\n\u001b[1;32m    194\u001b[0m                 _interpd_[interpolation],\n\u001b[1;32m    195\u001b[0m                 resample,\n\u001b[1;32m    196\u001b[0m                 alpha,\n\u001b[1;32m    197\u001b[0m                 image_obj\u001b[39m.\u001b[39;49mget_filternorm(),\n\u001b[1;32m    198\u001b[0m                 image_obj\u001b[39m.\u001b[39;49mget_filterrad())\n\u001b[1;32m    199\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAABCnUlEQVR4nO3dd3yb92Hn8Q8ebIDg3hSXqC1R05b3bLwT2/FIfG3TpE0zmqbNNUl797o2bdr04rumd2kvbZPLxWmdnTjx3vKesixbw5KoLVKUuPfCBu4PkA9JLVLiwOD3/XrpReDBA/AHkXy++G1LPB6PIyIiAhjJLoCIiKQOhYKIiJgUCiIiYlIoiIiISaEgIiImhYKIiJgUCiIiYlIoiIiIyTbdE28w7p3LcoiIyBzbEntoynNUUxAREZNCQURETAoFERExKRRERMSkUBAREZNCQURETAoFERExKRRERMSkUBAREZNCQURETAoFERExKRRERMSkUBAREZNCQURETAoFERExKRRERMSkUBAREZNCQURETAoFERExKRRERMSkUBAREZNCQURETAoFERExKRRERMSkUBAREZNCQURETAoFERExKRRERMSkUBAREZNCQURETAoFERExKRRERMSkUBAREZNCQURETAoFERExKRRERMSkUBAREZNCQURETAoFERExKRRERMSkUBAREZNCQURETAoFERExKRRERMSkUBAREZNCQURETAoFERExKRRERMSkUBAREZNCQURETAoFERExKRRERMSkUBAREZNCQURETAoFERExKRRERMSkUBAREZNCQURETAoFERExKRRERMSkUBAREZNCQURETAoFERExKRRERMSkUBAREZNCQURETAoFERExKRRERMSkUBAREZNCQURETAoFERExKRRERMSkUBAREZNCQURETAoFERExKRRERMSkUBAREZNCQURETAoFERExKRRERMRkS3YBRDJNSV4xt2z+EEW5hQz5hxkcGWLQP8TQ6NdB8+sgQ/5horFosossYlIoiMwSj9PNbVfcylX1l2I7j0r4SCiQCI/AMEOBYQYDI6NfhxkY6GNwoM8Mk+HAMPF4fA7fhSx0CgWRGTIMg6vWXsFtV9xCls153s/3OFx4HC6KcwqmPDcWjzMcCTAYDjIYCTAYDjDc3s5Afy9Dp9RCBv3D+IP+C3lLsoApFERmYGXVcu6+9eOUO3PMY2FLlN2Fx9mTfwJr3MAdceCK2nFH7LiijsTXiAN3dPJXZ2zqP0fDYsFnd+Ozu8cPFtSc9fxIPMZAPMhwdw+DI0OnBMf4/d6hPvqHB2byXyEZQqEgcgFK8oq55+aPs7q4ZtLxQzltvFt8jBF7EIAwUQK28LRe04hZxkMjasc9ITDG7rsidtyj59ji1ilf02YxyLe4yS+qmPLc9w7u5MdbfkkoEppWeSUzKRREzoPH6ea2K2/j6lWbsRrjF+V2dz9bSw7T6Rm84NeOGXFGjKAZKFOxxYxETWM0KFwTaiLj90fDJWLHmKKfY9Oy9RTmFvK9xx9QrWEBs8Sn2Wt1g3HvXJdFJGUZhsHVF13HrRf/1qR+gyFbgG0lRzma3QGWJBZwKnFwxGyTahoTayBL+0twjDZf9Qz28d3Hf8DJrtYkF1pm25bYQ1Oeo1AQmcLKquXc9ZH7qLBlm8fG+g12FzQTNWJJLN3syA14uKl5Lb6wCwB/KMADT/+IfU0HklwymU0KBZEZKMkr5u6rPsKa2lWTjp/ab5Ap3BE7NxyvpziQCL9YLMYvX3mY1z94O8klk9kynVBQn4LIKTxONx+++iNcteJirMZ4O/xs9BukMr8tzFM1O7n25EpqB4swDIP/dP09FOUW8cgbT2h+xAKhUBAZZRgG11x5M7fUX5me/QazIGrEeHHRXi7uWMy67ioAPrTxGopyCvj3Z3+qkUkLgJqPREj0G9xz/V2U5RSaxzKt3+B8Le8t44rWZRijSdjU3sz3nvihRialMfUpiExhofUbnK/yoTw+dGL1hJFJvXz38Qc0MilNKRREzsLjdPPh6+7kymUbsVkWTr/BhThtZFI4yANPPaiRSWlIoSByCsMwuGrNZdx21W2n9Ru8W3KUIwug3+BCnDYyKR7jly9rZFK60egjkQlWVi3nnhvupSwrzzwWsUTZtYD7DabrtJFJlrGRSYU88saTGpmUQRQKkvES/Qa3s6Z25aTj6jc4P2cemXQtRTmFGpmUQdR8JBnrbPsbqN9g5jQyKT2pT0EWJAsWrl57ufoN5tipI5N6/UP828Pf08ikFDadUNAezZJxbrjoOj5+3V1mIEQsUd4rOsZDS7ZxJEeBMFtasnp5vOZ9Bu0BAPLcWXzl43/KqurlSS6ZzIRCQTKKBQvXrL3CvH8op41fLdnGjqImdSTPgT7XCI/XvkeHK9Fs5LI5+MLtf8hV9ZcluWRyoRQKklFqy6rJ8+UCcDyrm1cr9qsjeY6NjUw65usEMNdMuvuq27FYVC1LNwoFySgblq41bx/L7kxiSRaWsZFJuwqOm8d+a+M1fPa2T+GwOZJYMjlfCgXJGBYsbFy5CYAoMZp8XUku0QJjgXdLjvJ62QFiJMavrKtbw5c/9kVyvNlTPFlShUJBMkZtWTV5riwATmb1ErJGklyihelAXivPVu0mZCT+/6uKKviLj3+JisKyJJdMpkOhIBljw9L15m01HSXXaSOTfLl85WN/opFJaUChIBnBgoWNy9cDajpKFaeNTLI7NTIpDSgUJCPUllWT5/EBajpKJRqZlH4UCpIRNOoodWlkUnpRKEjas2Bh45J1gJqOUpZGJqUNhYKkvYkT1tR0lNoO5LXy3Kkjk+7TyKRUolCQtKemo/Ry8tSRSVm5fOXeL7KqekWSSyagUJA0p6aj9HTayCSHi89/5PcpKyhNcslEoSBpTU1H6evUkUk2q417r74jyaUShYKkNTUdpbeoEeOVigYG7X4AVlQtY+3i1Uku1cKmUJC0paajzBA1YrxTcsS8f/fVd2CzWpNYooVNoSBpS01HmaPR10Wrpw+AopwCrlt/dXILtIApFCRtqekog1jg7dLD5hyGWzZ/iOzRGeoyvxQKkpYsWNg4ugCemo4yQ49riAN5if2dXQ4Xt19+S5JLtDApFCQt1ZZVk5eVA6jpKJO8V3SM4OjEtktXXUxV8aIkl2jhUShIWlLTUWYK2MLsKGoEwLAY3HPNnUktz0KkUJC0o1FHmW1f/kn6HCMALCmvZdOy9ckt0AKjUJC0o1FHmS1mibO15LB5/64rP4LdZk9iiRYWhYKkBbvVxvq6ei5fvZkbNl1nHlfTUWY64euhOasbSOzaduOEn7nMLVuyCyAyHXdecRvXbZg8dl1NR5lta8kRKobyMDC44aLreWvfNnoH+5JdrIynmoKkBZfTddqxY9mdajrKYP3OEfbmnwTAYbNz5xUfTnKJFgaFgqSF9w7uMm/3O0Z4sWIvb5QfSGKJZD7sKGrCbw0BcPHyDdSV1ya5RJlPoSBpYf/xg/QN9QPgC7lo9fYRMWJJLpXMtZA1wnvFx8z79159Bxa0t/NcUihIWojFY2zb/x4ABgZ1/cVJLpHMlwO5rXQ7hwCoKqnk0lUXJ7lEmU2hIGlja8N28/bSfm3GslDELbC1dHyI6h1X3IrL4UxiiTKbQkHSRltPO41txwEoDPjID3iTXCKZL63ePnMznmyPj1suviHJJcpcCgVJK1sb3jVvL+1TbWEheafkCBFLoh/pug1XUZRbmOQSZSaFgqSV7Qd2EI4khqEu6S/BElen40Ix5Aiwp6AZSGzdedeVH0lyiTKTQkHSykjQz+6jewBwRx1UDuUnuUQyn3YWHmfYFgRgXd0aVlQtS3KJMo9CQdLOxCakZWpCWlAiRpR3i4+a9++5+g4Miy5js0n/m5J2GpoO0j88AEDVYAGuiBZLW0gO57TT4U78/MsLSrmq/rIklyizKBQk7cTiMbY1aM7CgmWBt0sOmXdvu/QmrIY1iQXKLAoFSUuTRiFpzsKC0+kZpHF0iGqW20tZfkmSS5Q5FAqSllpPmbNwUUcty3pLWTSUT37Am2hSiie5kDKnxpqQAMoLy5JYksyipbMlbW1teJea0ioA1ndVn/Z4lBh+W4gRe4gRW5AR24SvE44FrGG0nE766XENm7crChQKs0WhIGlr2/73+dDGaynMKTjj41YMsiIusiKnL7s9UZQYI7Yww7YQw9YQI7YQw7YQIYdf4ZHCekbXQwLVFGaTQkHSViAU4O9+/A8sKiwnx5tNTlY22Z5scrOyyfZmk+PNJtebjc/jO+frWDHwRZz4IudeTydKjGEjTJt7kN2lR+lzjszm25HzNGILETDCuGJ2KhQKs0ahIGktEo3Q2H78nOdYDSvZHh/ZXh+53hyyvT5yvDmTwiPHm032NMIjO+Yke9jJkiMFHMxt5b2iRvz20Gy+JZkuC/S6hikbySU3KweP081I0J/sUqU9hYJkvGgsSu9QH71DfTTRfNbzzhYeY7WQHG82Bdn5eJxuDCys6Cunrr+EDwqa2V3YTMSIzuO7EoAeZyIUACoKyzh08ui5nyBTUiiIjJpOeNitNq7fcDU3XvRbuJ0u7HErG7tqWNFbzvvFxziQ20bcomFP86XHNaFfoUChMBs0JFXkPISjEZ7b/hJ/8+A3eXnn60SjidqBJ+rgytbl3HloM1WDBRoOO096neMjkNTZPDsUCiIXYMg/zEOvPsrf/eQfeP/Q+P7RBRE3NzbXc0vjBgr95+6jkJmbGArqbJ4dCgWRGejs6+IHT/+If/zVdzja0mger/DncOexTVx3YiVZoXMPiZULF7ZGGbQnOpfLC0q1f/MsUCiIzIKjrY3840Pf4ftP/gftvZ3m8bqBEu49vJnNbXU4o+rCmws9o7UFl8NFfnZekkuT/hQKIrNo55EP+MZP/oFfvvwwgyOJTlArBmt7Krn34KXUdy/CGtOf3WzqnTCzuVwzm2dMv50isywWi/Hq7jf5mwfv59ltLxCKhAFwxW1c0r6Euw5tTqzsqs7oWTFxZnNFoRZHnCmFgsgcCYQCPP72M3z9wft5e982YvHE/sI5URfXnVzFHcc2UTacm9xCZoBJayAVliexJJlBoSAyx/qG+vnxll9y/8/+N/uaDpjHiwI+bmtazw1N9eQGPUksYXrrd/iJWhKBW16gmsJMKRRE5snJrlb+5dHv851H/i8nOlvM49XDBdx15GKubFmGO+xIYgnTU9wSp8+RWIeqOK8Im1Ub7syEQkFknjUcP8j9P//fPPj8z+kd7AMwl82499AlbOyowRbThe18jM1sthpWSvO04c5MKBREkiAej/NOw3a+/uD9PPbmU/iDAQAcJJbN+NihS1jRW4YlrnH309Gjmc2zRqEgkkRTLZvx0SMXa9mMaeh1aWbzbFEoiKSAsy2bkR/ycGNzPbc2rteyGecwacMdzVWYEYWCSAo527IZ5f5cLZtxDmMb7oBqCjOlUBBJQedaNuOew5u5RMtmTGYZb0Ia23BHLoxCQSSFnWnZDBsG9T2V3HvoUuq7KrVsxqjJM5tVW7hQ+m0SSXFnXTYjZuOSjjruPryZmoHCJJcy+TSzeXYoFETSxNmWzciOuPjQiTVUL/BgmLThjmY2XzCFgkiaOduyGVe3LF/QndDahW12KBRE0tTYshnvHdwJgDNm5/oTqzAW6IQ3bbgzOxQKImnupy8+REdfFwDFgWwubl+c5BIljzbcmTmFgkiaC4QCPPD0jwhHIgDU91Qu2P6FHs1snjGFgkgGaO48yW9ef8y8v1D7F3o1s3nGFAoiGeK13W8t+P6FIn+2edtp1zLkF0KhIJJBFnL/wqLBfOp7KgGIRCNsP7gjySVKTwoFkQyyUPsXPGEH17SsMO8/8saTnOxqTWKJ0pdCQSTDLLT+BUscrj25Enc00Vy0++heXt75epJLlb4UCiIZaCH1L6ztrKF8JDH8tHewjx9v+UWSS5TeFAoiGWoh9C+UDuewqasaSKwR9cNnf8JwYCTJpUpvCgWRDJXp/QvOiJ1rT67CGJ25/NQ7z3Gk5ViSS5X+FAoiGSxj+xficHXLCrIiTgAONB/i2XdfTHKhMoNCQSTDZWL/wuqeRVQPFQAwODLIfzz3M+JxbWQ9GxQKIgtAJvUvFPp9bJ5Q/gef/zn9wwNJLFFmUSiILACZ0r9gj1q5tnkV1tFL15btL09aPlxmTqEgskCkff9CHK5oXUZuJLH/8rG2Jh5/+5kkFyrzKBREFpB07l9Y1lfKkoESAEaCfn74zE+IxqJJLlXmUSiILDDp2L+QG/RwWetS8/5PX/gV3QM9SSxR5lIoiCww6da/YI0ZXH9iFXasALz+wVvsOLw7yaXKXAoFkQXoTP0L+QEvpOCozkvbl5AfzALgZFcLv371sSmeITNhS3YBRCQ5Xtv9Fksr6ti0bD3OmJ27jl7MoBGizdtHu7ePNk8ffc4RkrXVsSUOS/tKWdlbDkAwHOSBp39MOBpJToEWCIWCyAL20xcforK4guLcIgB8MQe+wWKWDhYD4DfCtHv7aff00erpp9s1RNwyd9UJZ9RGxVA+VUMFVAwU4I6PX6J+9cqjtPV2zNn3lgRLfJrTAG8w7p3rsohIEnicbq5eewVLF9VRW1qNy+E867khS5QOTz9tnn7aPH10ugeJGrEL/+ZxyAt6qRzKp2qwgOKRHAzL6VWTdxq28+DzP7/w7yMAbIk9NOU5CgURMRmGQWVRBUvKF7OkYjF15bVkub1nPT9KjE7XIK2eftq9fbR7+glbzz1M1BozKBvOo3KwgKqhfHyRM8+V8Af9NBw/yK4je9h+YAfxVOzwSDMKBRGZEQsWSvOLWVKRCIkl5YvJ8+We9fw4cTptI3Rk99LmSTQ7+W1hvGEnlYMFlPcVURnINkcSnaqtp4M9x/axp3Efh1uOEYvNoBYip1EoiMisK8jOZ0l5LUsq6lhSUUtJXvE5zx+yhsga3RXtVOFIhMMnj/DBsX3saWygq7/7tHNsdgO324HLbcflseP22HGN3k/ctuP2THjcnTg2OBDglw9sY3AgMCvvOxNMJxTU0Swi56V7oIfugR7e2f8eANkeH3XltWZQVBSVYVjGR7ufGgh9Q/3saWxg77EG9jcfJBgOTXp8zcYK7vztjWTnunF77NhsZ65VTMfQQJBfPPDOBT9/IVIoiMiMDIwMsuPwbnNCmdvhYnF5jdkvUV5QSltvB3uONbDn2D5OdLacs3/gtz9zKXkFZ+/HOB8bLqnilz/cpmW1z4NCQURmlT8UYG/jfvY27j/v515z0/JJgdAVCBOKxgnF4oRjMUKxOKHo5NunPhaKxbi6LJtan4vsXDdLVxVzcG/7bL7FjKZQEJGUYLHAzR+tN++/2jrA/j7/Bb3WkYEAtb7EqKaNl9YoFM6DlrkQkZRQt7yY3HwPAM1DwQsOBICmwRCRWKLJaMMlVVjOMPdBzkyhICIpYeNl1ebtQ/0zHzE0EErMl8jOdVO3omjGr7dQqPlIRJLOYoENlyRCIRqL0zQUvODXqs5ycGVJNlmO8VFLVkOff6dLoSAiSTex6agrGMFhWAjHzm8Os8dmcEWJj8XZ4zOkQ6EIT/5yFwf2ts1yiTOXQkFEkm7NxkXm7RK3nd9ZWkQ0HmcwFGUwHGUgHGUgNP51MBwlPNpnYAFW5bnZXJSFwzpeI9i3q4Vf/OAdOtsH5/vtpDWFgogkXV/PyGnHrBYLuU4buc4zX6b8kRiDoSg2A/JddvP4QJ+fhx58l+1vNs5VcTOaQkFEku6VZ/fTfKyHRTV5FBZnUVjiG/2XhWvCBX8it83AbZvcV/D6Cwd59KfvMzIcOuNzZGoKBRFJCUcOdHDkwOn7JfiyXRSWZFFYnAiJsbAoLPaRm+/BMCy0HO/lZ//vnTM+X86PQkFEUtrgQIDBgQDHDnWd9pjNbuDxOhmYwZwGmUzjtEQkbUXCMQXCLFMoiMicsNkMnE4bmkycXtR8JAuGzWrFaXcxHBhOdlFSmmEYOGwOHDY7DruDnsHe89rsprQih1s+tpFNmxdhtSYSIRCKEgrHCIZjhAZGCPjDBAMRQsEIwUCYYDBCMJD4Fwqcemz8dk/nEP6R8Fy9dUGhIBnO6/KwpmYla+vWsKp6BXabnRfff4VH33wqY5ZTLs0rJtubjcNuT1zM7Y5JF3WnzYF97DGbHafdgd0++rh5vt38arNOviz0Dw/w9z/5FsOB04eNTlS1uICbP7rGnJk8kcthxTU2wzj37HtATyUUivIv//0FDjVogbu5olCQjFOYU8D6lZuor15BXXElxilLHNyw6TpWVC3j27/+VwKhC19OIVksFguLy2pYv3Qt6+rqKfTlzen3y/FmU5hTcNZQWLqyhJvvXsuqtWWTjgciMbqDYWyGBbvFSHwd/WczLqxNyeGw8skvXsHff/UJAn7VGOaCtuOUtGfBQnVJJeuWraN+5XrK3blnPC9iiWGLTw6IXv8QHxzaTUt3G63drbR0t035iTgZrIaVZYuWsH5JPeuWrSPb6ZmV141YokSM2PhXI0p+IAuD8Yv2W3vf4Scv/Oq0567eUMEtH9tAXV3+pOPD4Si7ekZo6PUTOcvlxQJmSNgs42ExFhiTbo8+Xu51UDg6Z+Gtlw/z4+++NSv/BwuJtuOUjFZZvIgr11xK/fJ15DrOfJHsc4xw3NdFk6+bfscIv3vwikmP57mzuHrt5ZOO9Q8P0NLdRkt3K63dbbR0tdHa004wPL+1CofNwaqa5ayvq2f18nq8ltP3OY4Ro9XbR6drkIgxfmGPWGLj9y0Tj08+Z8K1H0scLm9bSmHAZx57bvtLPPbmU+PnWCxsuKSKm+9ZR2VV7qSy9Ici7Ooe4UC/n9gUHzXjQDgWN5eqmI4sm8E9iwtwWg0uv24Ju7c3s+vd5mk/X6ZHoSBpyefJ4sv3/jFO2+QLZZw4He4BmsaCwDn5U//TVTu59fj6c752jjebHG82K6uWTTrePdAzGhZttHYlahVtvR1EopFZeU8AHqeb+tpVrF+ylpU1K3BYT/8TjViinMjqodHXRbOvm6B15t/fGjO49uRKagfHl5j+9WuP8dKO1wAwrBY2X7mYm+5cQ2lFzqTn9gQj7Oga5shA4LwWsDtfQ5EYb7UPcl154vv/zmcv5eiBTgYHZr7MtoxT85GkpRsvup47r7gNSFwkT3p7aRq9SPpt525rtsYMLuqopb6nctLxI9kduCN28oJe3NHTP5WfSSwWo3Owh5a+Tlpaj482Q7XR0ddFLD69ETs53mzW1dWzfsV6lpbUnHGZ56ARodnXTaOvkxNZPUSM6Y8Gmoo9auWG5nrKR3IBiEaj/GjLL3j3wPvY7VYuv34JN96xhvzCyfsmd/jD7OgapnEGy1xfiBsX5Zi7qu18t5n/+62X5/X7pzM1H0lGsmDhitWXmPcfrtvOgGP6E5iiRox3So/QkN/Cxw4nXmfYFuTV8gZiRuIzkms0HPIDXvKC4/8cscl/MoZhUJJTSElOIRuqV5rHw7Eo7SN9tJxoMvsqWrrb6BnoJU6copwC1tXVs2H1RdTmT+6gHTNiDdGU3UWjr5NWbx8xy+x/DndHHNx0vN5sMgqGg/y/px7kaMdRbrh9NR+6fQ3Z2ZNHC7UMh3i/e5iTSVpf6LXWAUrdDtw2g/UXV3LpNYvZ+urRpJQlEykUJO0sr1pKUW4hACe8PecVCBMNOPz8YNUreMNOQkbEDASAgC1Mq62PVm/f+BPi4I04yZsQFPlBL7lBD7a4ddJr2w0ri7IKWLSiYNLxQCjIkH+IwpzJx80y2f00+rpoyu6kwz1AfA4nfvlCLm4+soGceOKiP+Qf5odb/oMlm318+ta78Xgn15aaBoPs6B6mPcmjfgLROK+2DnBzZS4AH/v9zRzc205Pl+afzAaFgqSdK9dcat7en9cy49cbtk+z+cOSOHfYHuSEr2f8cBx8IfcpQeElN+jGOGXRAJfDicsx+ZN3j3OIRl8Xjdld9DiHJnX+zpUCfxY3Na7DE0+M5ukNdrI/9iaf+8Zl4/MJgHg8ztHBIDu6hukOzl7fyUw1DQU50Odnea4bt8fB733hcv75G1vIkKknSaVQkLSS7fGxrm4NMNq84utOcokgboEBp58Bp58mxhdtM+IWcoKeSc1PeQEvWREnXa5BmnxdNPq6GHDO79o9+QEvtzZuwDlau+nzHCX7kn4ut9Wa50TjcQ71B9jZPUz/6F7Hqeat9kHKvQ58divL15Rx7c0rePmZ/ckuVtpTKEhauXTVxViNxMXsYG4r8TloZ58tMUucXtcwva7Uatao6y8xA6Gp9yiLNvdhsyXuR2Jx9vf52dU9zFBk9jqz50IoFueVln4+Up2YJ3Hn72yiYXcrbSf7k1yy9KYF8SRtWLBwxforzfsH8lqTWJr01eke357SUd2DzZ4IhCMDAX52uIs32wdTPhDGtIyE2d2dCF2Hw8on//gKDKtW4JsJhYKkjeVVSynyJsaon/D2MOjQ+PQL0ZzVTdgSBUeYstWJPoVwLMYbbQP4o+kRBhNt6xyid7S/o2ZJIbd8tD7JJUpvCgVJG1fWX2beno0O5oUqasRo9nVDRTuW0SvAnh4/gWjqNsWdSzQOL7X0ExvtZb7l7rVULT7z6C6ZmkJB0kK2x8e6xauB1OlgTmcn8zqgNDGCKhaF3T2p1e9xvroCEd4fHZJqtRp86otXYLdbp3iWnIlCQdJCOnUwp4PCghEso/Myoq0FpNBo0wu2o2uYjtE5FGWLcrn+tpVTPEPORKEgKe/UGcz71cE8Ix6LheWuxMS0eNTAdrKU8sjcLr89H2LAnp7xta7cnuktVSKTKRQk5Z06g3lIHcwzssLpxDq2R2ZrAURsLA6VJrdQs6TEbTdvH9rXlsSSpC+FgqS8+tpV5m1fyE3tQBFzuhxnhvNMXHCvJ7HmUU2oGMtcrqkxT8pHl+aIRmIc3t+R5NKkJ01ek3lnWCyJrR/tia0hx7+ObRc5uo3k6GOlhXnkZruwWCwUWDzUDRcQDEbodQ8TskWwY8UWt9JtDPGSvYEBQzWJ6bLmdBId9OGOOyiP5HHS3jP1k1KUx2aQ50xc0hoPdxEMZEBHSRIoFGRO2G12PnXLXVy8ci2u0Qv+2MXebpudX7sSfDBhBYalsWLWRRbxmHMnO63N87KGULqz5rcRPbEYgMWh0rQOhYoJfQj796jf6UIpFGTWFeTk8je//6esqFo879/bjYP7gptZZS3nUecORizJWd45lcUmrBpnzekBIwIxGzWhYt7wNKTtyK6KCau6Htij/oQLpVCQWbWiuo6/+dQXKchJjGaJxmIEQkGi0QiRaHTCv8ikr9GzHI9EIxMeGz+e482lqrgGr9NLLB7HYoHCPC8+b2IF0rXRRdT6C3nY8T4NNn1qnKgjGmX16O2oxYutsIVIR1XaNyGN9SeEQhGOHepMcmnSl0JBZs0NF1/Jl+79JA5bYgTIwPAgj7/xIl39vbP+vdp6OjnQfIjy/HKWLVpBljuL9q4hhkZCFOd7sVoNfHEXnwxezvZoI084dhG0qI0ZoCkUIuLxYLNYiER92IuPE+moAtK3CSnbbsU3tobT/g4i4fRbriNVKBRkxgzD4LO338ddV99oHmvuaOXpt1/GH5zbrRpbelpo7WmlonARyyoSeyofD4QpLsjCO9rGfFGkhrpoEQ853+OoVZ8gw8CJcJgah4M4doz8trRvQlLT0ezRkFSZEZ/Hyzc/8+VJgbDzUAOPvPrcnAfCmDhxTnQ18/Lul9h9bBfDgRFaOwdp7xoiFkt8YsyLe/ls4Go+HFyLLa5f+2Ph8d3ToiSakACzCSndlE/sZP5AzYUzoZqCXLDqknK+/ukvUVFYAiQ2fH95x1b2HD2YlPLE43GOdzRxorOZ6pIalpQvxR8IU1yYhceVaNK6MrKUFZEyfuHaxgnr7DdrpYtMa0Iaqyn4R0I0H0uvsqcafWSSC3LZ6g3885e+ZgbCSMDPb159NmmBMFEsHuNY21Fe2vkCHxzbQ+PJLjp7honFEk0ihWTxBf913BJYgzUDJmxdiLEmJGC0Cakv0YRE+k1ky3facNsSl7JD+9rNn7NcGNUU5Lz9zg2388lb7jLvd/R288SbLzI4klorbUZjUY60Hqapo5HFpXUsr1pKRXEeLqcNw2Lhmuhy1g5W8jP3VprtC6/WcGy0XwHGm5DScRRSuWd8aQs1Hc2cQkGmzeVw8NX7PsPV6y82jx04fpQt775BJJqa+/gCRKIRDp48wLH2Yywpq2Pj8jUU5nmxWCzkWT38UfA6dg2d5BHfdkK21H0fsy1TmpAqRochgzqZZ4NCQaalJK+Qr3/6T6krT1w04vE4b37wHtv3f5Dkkk1fOBKiobmBo21H2bBkHRuWL8fpSNQaNjgWsWygmBfjDWzLO0rEyPwhjZkwCsnCeE1hsN9P64m+pJYnE6hPQaZUX7ec7/zZX5uBEAyFeOyNF9IqECYKhoNsbdjGj579DU2tbcRHZ/h6nQ4+7FjLZ05cy6qeCoxY+rSrX6h0H4VU5LLhsCYuYwf2tBFP7QxLCwoFOacPX34d//Pzf05uVjYAvYP9/OLFJ2lsPZHkks3ccGCER15/hifeeAl/IDF81jAsVOblclXnMj52+BLKhnOTW8g51hQKERm9kkaiPqzFx83H0mE57XLNT5h1CgU5I5vVypfu/SR/es8nsVlHV55sPcEvXniS3sH+JJdudh1tPc4Pn/4VR1uagUQw5GW7yYq4uOH4GvIC3iSXcO6k+yikiYvgKRRmh0JBTpOb5eN//tFfcNtl15nHtu//gMfeeIFgODMXmAtHImx59w3CkcQF0ed1YrVacMRt3Nhcjztin+IV0le6NiFZLVA6GgrdnUN0tg8muUSZQaEgk9RVVPEvf/Z16hcvBxIjd57d+ipv7N5utr1nKn8wwK7DDUBi83erbTQgwi5uaK7HGsvMP5dzNSEtD1Ykq1hTKnbbsRmJmoxqCbMnM3/L5YJcs34z3/6Tv6Q4rwCAoZFhHnrpGfYfP5rkks2f9w7sMWsLJQW5DPkTnz6L/dlc07IiI3d8O70JqReLPbFR0eJQCZ6Y8xzPTp7JTUeanzBbFAqCYbHw+7fezV/+3hdwORIXgNauDn72whO093YluXTza2JtwWl30D5wgkBo9AI5UMymzpoklm7uTGxCCsUKsVUfBsDAYFVgUbKKdU5aBG9uKBQWOI/Lzdf/4Ev8pw99xDy299hBfv3KM4wE/EksWfJMrC1cu2EzD736iLmw3oauGpb0lSSzeHOiKRQiOPoeY3EP4eII1J4Aa5SVwUqMFOtwthsWityJfp62k/309y7M39W5oFBYwMoLS/g/X/oal65eD0AsFuOVHe+w5d03icYyf/LW2ZxaW1i3dCm/ef1x8/GrWpZTOpyTrOLNiTDw/PAwg6Mz0y0WC5bybtjUgLt4kLpQagVhqduO1TLWn6Cmo9mkUFigNi1bzXf+819TVVIOQCAY5JHXnmfnoX1JLllqmFhbuO3y69h1ZDev7noTACsGHzqxhuyQO5lFnHVtkQgPDQyw3e83O54t9iiWJSe4aomdEldqjMDy2gwuLsoy76vpaHYpFBagTctW83d/+Gf4PInx9139vfz8hSdo7tAnrjGn1hY+dv2tPPTqo+xt3A+AK2rnxuP1OKOZtVJMFNgRCPCr/n6OBMeHH9uzQtxZm8/1pdl4bMm7bFR4HNxTk282HQ32+2nYrd/b2aRQWGBWVC3mr3//T7DbEhezIyeb+OWLT9I/rDHepzq1tpCb5eOBZ35MS1fiIpQb8vBbzatTrr19NgzH47w0MsyOVhvxYZd5fGmem48vLmBdvmfeLx4bCjzcWpWLa3Tbza6OQb7zzRcJ+MNTPFPOh0JhAakqKefvP/Nl3M7EH/nhE008+dbL5oVPJjtTbSEQCvBvjz/AwEgiRMtH8riidVlGDlUFeN/Rhf+DGuJHKoiHExdjh9Xg0hIf9y4uoHLCCKC54jAs3LQoh83FPozRfoQP3j/B/f/lKW2oMwcUCgtEcV4B93/uq2R7E22xze2tPLP11YyfkDZTp9YW8n059Az28r0nfkg4kviEuryvjLXdlcks5pyJGXH257VCWyG8v4KmXSPmJja5Thu3VuVx86Jcskc/vc+2fKeNu2ryqfElPsjEYnGe+OVOvvs/X2JkODNn1yebQmEByPH6uP9zX6UoNx+A9p4unnjzRaKxhbN3wIU6U20BoLHtOD96/hfmeZs76qgZKExKGedaQ14LMWIQsZHXsYZv/eUzHG5oNx+v9jn52OICNhdlYbPMXlPa0mwXH63JJ8eZaOocGgzyr/e/yNO/2a3VUOeQQiHDeZwu/vtnv0xlcRkAPQP9PPr684QiaoedrjPVFgDeO7STx996xjzv2pMrKfT7klLGuTRiD3EsuxMAn8dHib2a//U3z/HAP79Gb3ditz2rYWFDoZf7FhewJNt1rpebkmGBK0t9XF+RYy5j0XSki/v/65Ps29UyszcjU1IoZDC7zc7X/+BPWVZZC8DgyDCPvPYc/mAwySVLL2erLQA8++4LvNOwHQBb3MqNzWvwhlNzWYiZ2Jt/0rx97fqrANj+ZiN/+58f45mHdxMOJ2qdXoeV36rI4fbqPApd5z8yy2szuKMqn9V5HvPY6y8c5B//+ll6OlNru9dMpVDIUIZh8N8+8XnWL10FJC5sD7/6XMrto5wuzlZbAPjpi7/i0MkjAHgiTm48Xo89Ojdt7MnS4R6g05XoXK8uqaS2tBqAYDDC47/Yyd99+TF2vdtsnl/mcXBXTT5XlfpwWafXpFThcXB3bQHFozuphUNRfvRvb/Kz728lEl64kynnm0IhQ/3nez/FFfWbAAiFwzz2+paM2wdhPp2rthCJRvn+kw/S0ZdYJ6ogmMW1J1aR4jtZnh8L7M0f31hprLYwpqt9iO9962W+899foL0l8XtmsVhYlefhvsWFrM5zc65oGBtu6h6dA9HVMci3vvYMb79yZNbfipybQiEDffrD93LzJVcDEI1GefKtl2jrWVgL282Fc9UWhgPD/NtjP2AkMAJA9XABl7UtTTQlZUg4HM3uwG9NjPjZuGQtOd7s087Zt6uFb3zlCX7z4+34RxLnOm0GV5Zmc3dtvrmf8hiHYeGmCg03TSWW+DTHJN5g3DvXZZFZcO91t/CZj3wcgHg8ztNvv8KhE43JLVQGuXLtRVy0oh6Ah199ju899vNJjy9bVMef3Pk5rNbx5qOQEaHPOUKvc5g+5zC9o7eHbUHO+fE5BW3qqGFDVw0AT7+zhSe3PnvWc7NzXNzx2xu5/Lolk44fGQiwtX0Qh9XgxkU55DgSfQ+xWJwnH9rJsw9/oNFFc2RL7KEpz1EoZJCbNl/FV+77tHn/xe1v8cHRA0ksUeZxO138wW33YrfZCIZDfPLv/5yeU5rlLlu1mU/c8PEpXytkROhxjNDrHGHANZQWYeEJO7jv0KUYGAyODPKXP/wGkei5hzbXLCnk43+wmZol40N2I6NzHcZGFw0NBvnh/3mdBo0umlMKhQXk8jUb+dqnvojVSLQIvvnBe7zbsDvJpcpMU9UWAOrKa6mvXUVZQSll+SUU5hRM+/WDlig9Nj8D3sGUrFlcd2IldQOJVVMffO5nvLP/vSmfY7HApdcu4aO/vQFfzuSFBJuOdPH9//2qRhfNg+mEQmat5rVAra1bwX/7xB+ZgfD+wb0KhDn03oE9rFuyErvNxm2XX8d/PPMwgdDkYb5HWo5xpOWYed9hc1CaX2yGxLnCwhm3UhbOoqwva9LxEFH63MN0O4do9nVz0ttL1JjnUTlxCNjG57hsWrZ+WqEQj8PbLx9m5ztN3HrPOq67eQVWm8HrLxzkV/++TaOLUohCIc0tqajmbz/9JRz2RAfevsbDvLZzW5JLldn8wQD7m45QX7c8sd/CkhW8s2/XOZ8TioQ43nGC4x0nJh0/n7BwYKXYn02xP5uVfeWELVGafd00+rpozuombJ3bGerOiJ1rW1ZQOTRetraejvN6Df9ImN/8aDsvPLEXb5aTlua+WS6lzJRCIY1VFJXwzc9+Ba8rUR0/2nKcF959I8mlWhiOtZ6gvm45AJuW108ZCmczk7Cwx60sHihm8UAxUUuMZk8vx7M7afJ1E7TN7oz1kpEcrjuxiqxIYmJeLB7jmW0v8PQ7z1/Q6/X3+rVbWopSKKSpwpw8/sfn/pxcX2JY4InONp56+xViGrYxL050tBKNxbAaBhctXzPrr3+2sHDaHSytqGP9knrWLl5DljuxJ4Y1blAzXEDNcAGx1jhtnn4asztp9HUxYp/BDPY4rO2u4qKOWozRDo2BkUH+/dmfcqD50IW/rqQshUIa8nm8fPNzX6UkPzGao7O3myfeeJHoFKNAZPaEImFauztYVFTKouJSSvML52UuSDAcYk9jA3saGzAsv2ZJRS3r6upZX1dPni8XAAML5SO5lI/kcnnbUjpcA2ZADDin/+n8TM1FB5sP88Nnf2IuHS6ZR6GQZlwOJ3//mS9TU1oBQN/gAI+8voVgWMsIz7emtpMsKioFYNPyNTz19ivz+v1j8RgHTxzh4Ikj/PrVx6gqWcT6JWvZsKSe4twi87ziQDbFgWw2d9TR4xyi0ddFY3YXPc6hs45mKhnJ4foTq/CeoblIy61nNoVCGrFZrfz1p77Iyuo6AIb9Izz82nOMBNQ2mwzH21rMpUQ2JiEUJooTp6m9mab2Zh578ynKCkpZX1fP+iX1VBZVmOflB7PID2axsauGfluAxuxOmrI76XAPJAIiDmu7K7moY7GaixYohUKaMCwW/uK3P2uOjw+Egjzy2vMMDA8luWQLV0dfN/5gALfTxYalqzAMg1gsNYZWtna30drdxjPbtlCYU5AIiLp6FpfXmOfkRFys66lkXU8lQ0aI4zmd+MIuNRctcAqFNPHHd32CazdcAkA4EuGx11+gq783yaVa2OLxOMfbW1hetZgst4cVVYvZ13g42cU6TVd/Ny+8/wovvP8KOd5s1tWtYX1dPUsX1WE1EstxZMUcrOodr1GouWjhUiikgd+7+aN85IrrAYjGYjz11su0dp/f+HCZG01tJ1letRhI9CukQii4PXYW1eSTX+ilr3uEjrYB+npGiMehf3iA13a/xWu738Lr8lBfu5p1dWtYVb0cuy0x10XNRQubQiHF3XnVh/jdG+8w7z+/7XUa206c4xkyn5rax9fquWhFPT9+7tF5/f6+HBeVNflULc6nsq6Iyuo8ikqyTjsvFIrQ1TFMR9sgnS39dLQO0Nk2yIG2vbyz/10cNiera1ZQkJ3POw3b1Vy0gCkUUtj1Gy/jCx/9XfP+Kzu2cuD40SSWSE417B+hq7+Xwpw8llXW4vN452wjo/xCL5W1+VTW5lO1uJDKukJyc6a3y5vDYaN8UQ7li3KARZMeC4WjdHX56WjuobNtgLW5pXS2eSfVMGThUCikqM0r1/HV/zS+4unWvTvYeaghiSWSs2lqO0lhTh5Ww2D90lW8vuvdGb2exQLFpdlULs6nqraAypp8FtXmk+WbOgDCsTjdgTBdgQgD4Shem0GOw0qOw0a23YrVOH0MqsNupbwsi/KyM9UwonS1DdDRNsh7bzey/c3GGb03SX0KhRS0unYpX/vUH2OzJn48uw43sHXvzuQWSs6qqe0km0ZnNV+0fM15hYLValC2KGe0BlBAZW0+i2rycLnsUz43GI3RNRoAiX9h+kPRs+7pY4HRkLCR7bCaYZHjsJ49MBxWyqvyKK/KY+1FlZxo7KXtpHbwy2QKhRRTW7aIb/zhn+G0OwA4cPwor+x4J8mlknM52dVOJBLBZrOZ4XAmdoeVRdV5VNYWULWkiMqaPMoqsrHbp97PeSQSpTMQMWsBXYEwg+e5smgcGIrEGIqEODky+bGJgZHjsI6GxuTAMAwL9RsXKRQynEIhhZTmF/HNz32VLLcHSHwCfW7b6xoSmOKi0Sgnu9qpLq2gOK+AyuIymjtazcdLyrL5xBcup3ZpEcYZPo2faiAUnVADCNMdjDASmdv5D+cKjByHlfvqEkuqrFpXzpYn9s5pWSS5FAopIs+Xw//4/J9TkJ0LQGtXB0+8+VLKTIaSc2tqO0n16NIjF62onxQKl12/hLrlxac9Jx6P03dqAAQiBGOp9SGgPxRlMBTF57BSt7IYh9NGKBhJdrFkjigUUoDX5eGbn/sK5YWJC0dXfy+PvrGFSFR/eOmiqe2keXvT8tU88tr4ktLDA+OrlLaOhDgyEKArEKE7ECGSJrXA5uEgqxwe7HYrS1eVsHfHyamfJGnJSHYBFjqn3cHf/eGXqCuvAmBgeJBHXnueYEgL3KWT7oE+hkaHoq6tW2FOBANoPDy+empPMMLeXj/t/nDaBALAieHx38dV68qTWBKZawqFJLIaVv7y975A/eLEZi0jAT8Pv/o8w/6RKZ4pqWhsIpvL4WR17RLz+PGj3WYzYPE0RhWlopPDIXOvDoVCZlMoJInFYuEr932aS1evBxLr5D/y2hb6hgaSWzC5YBObkC5aXm/eDgYjtJ5IjNjJd9mwTt3XnHJCsTjt/sRubqUVOeQXepNcIpkrCoUk+dzt9/Ghiy4HIBKN8MQbL9LZ153kUslMHG9vMUeKbVoxeWhq05HEz9ZqsVCYprUFNSEtDAqFeVZdUs63vvBfueuamwCIxWI8/farnOhsS3LJZKYCoSDtvYn+g7ryKvJ9OeZjE/sVilzpOb6jeWi8w3zVeoVCplIozBOP08Vnb7+P7331G6xbssI8/sL2NznacjyJJZPZ1NQ2vkDexuWrzdsTQ6HYnZ41ha5AhMDofInla8qmNedC0o9CYR5sWr6GB/7r/dxz7c1YrYnZq31DAzz62vMpsdSyzJ7jk4amjjchnTzeSziU2EM7XUMhzngTksfroGZpYXILJHNCoTAP/uqTf0xBTh4AkUiEt/a8z4+ffZTGNo31zjSt3R2EwokO2U3L1mCxJD5Nx6Jxjh9L9CvkOGw40/RT9onh8Sak1esrznGmpCuFwjwYGB5fm/6R159n275dRGPRJJZI5kosHjdnM+f6ss35JwBNE/sV0rS20KzO5oynUJgHz73zunm7slh/SJlu4tDUNYuXmbcbD4+PLitK0xFII5EY3YFETahqcQHeaSznLelFoTAPnt32OtFoomawpnap2aQgmalncHwV0YKcXPN2JnQ2w3i/gmFYWFlfluTSyGxTKMyDnoE+tu7bCUCWx8vi8srkFkjm1EjAb97OyxofltrZPsjw6LDOYnd6DkuFyU1IK9WElHEUCvPk6bdfMW9ftnqjagsZbGIo5Pp8kx5rGm1C8tisZNnS78/PYVjYUDA+m7mw+PTd2iS9pd9vZZrafmCPub9yYW6eud6RZJ5AKGiudTSxpgDQeCR9O5uzbAZ31ORT4U1sABUKRXjm4Q+SXCqZbQqFeRKPx/m3R35q3r9szQacDkcSSyRzyR8MAIkRSBOla79CgdPGR6vzyXcmmr0GBwL8098+z/4PWqd4pqQbhcI8amg6wovvvQWA2+nistUbklwimSsjY6GQNTkUJg5LTZcVUyu9Du6ozsPjSEy8bG8Z4B/+8mmOHeqa4pmSjhQK8+yBJx8iEEx0Nq6tW2HutCaZZaxfwW6zmdurAgz0B+juHAKgyG0j1XuWVuS6ubkyF7s1cak4sr+Db33tGbrah5JcMpkrCoV51tXfyy9efBIAwzC4ZsMlSS6RzIWxmgKcXlsYa0KyGwa5Tuu8lut8XFzk5ZqybIzRQRHvb23in7+xheHB4BTPlHSmUEiCX7/6LG3dnQBUlZSzeMKsV8kMk4al+k5tQhqfxJaKTUgGcH15NhsLx0cWvfDEXn7w7VcJhzUTP9MpFJIgFA7z/Sd+ad6/ev3FWA39KDLJpGGpZ6kpQOqNQHIYFm6tymNpjhuAWCzOr/59G7/58Xuk0e6hMgO6EiXJG7u3s/NwA5C4aGxYtnqKZ0g68U9sPvJNHpaaqttzZtkM7qyePOT0+//rFV5+Zn+SSybzSaGQRN995KdERy8Om1euw+NyJ7lEMlvO1XwUDEZoS7HtOQucNu6sySfPNXnI6a53m5NcMplvCoUkOtZ6gqfffhkAh93OlfWbklwimS3n6mgGaEyh7TnHhpx67RpyKgqFpHvw2UcYHBkGYFXtUkrytXFJJjhXTQFSZ3tODTmVUykUkmxgeIgfP/eIef9aDVHNCFPWFFJgZrOGnMqZKBRSwBNvvmyuwV9WUMyKqsVJLpHMVCwWIxBKXFxPXeoCkrs9pwFcX6ohp3JmCoUUEI1F+d6jPzPvX7n2Iuy29F1aWRLGmpDOVFNI1vac5pDTPA05lTNTKKSI9w7u5a097wOJPRcuXrE2ySWSmRprQvK63Djsp9cG5nt7ziybwR0Th5wGNeRUTqdQSCHff/wXhCMRADYuX022V2vVp7OzbbYzZuL2nHPdhDQ25DR/bMhpv59va8ipnIFCIYW0dHXw8GvPAWCz2rjrmps0GimNTZ7A5jvt8ckjkOYuFM445PSvnpn0/UXGKBRSzM+3PEFHb+ITZG5WNh+7/jY2LV+T5FLJhZiqpjAf23NqyKmcL4VCihkJBvjKv9zP3mOHALAaBletu5iPXn2jZjynmZHAxJrC6Z3NMHfbc1qAy4t9GnIq502hkILae7v46r/+D3625XFzjZzq0gp+98Y7qCmtSHLpZLpGgmdfFG/MxO05NxdnsSzHRZHLhm0Gg5GchoVbK3OpLxjfx0FDTmW6NO4xRUVjUf7jmYfZcWgf/+V3PkdhTh4el5s7r76R9w/s4c0P3jPXTZLUNJ2awsR2/aU5bnN1UoCBUJTDA37e6xxmuj/pPIeVmypzyXEk/rQjkSi/+ME7vPnS4fN/A7IgqaaQ4nYd3s/n//FrvL1nh3ls4/I1fOz628766VNSw8SaQt5ZflYNu1o4drDzjI9lO6xsLMzijpp8fPap/1Srsxx8tDrfDISBPj//9LdbFAhyXizx+PSmrNxg3DvXZZEp3HHlh/jM7R/HYUuMVAmFw7z8/lYamvRHn4rsNht/fNcnANh5aB9/8d1/OOu5eQUeyhblUlaZS9miHMoX5VK5OB+bLTFiKBiN8UrrAI1n6Q/YUODh4qIsLKP9B8ePdvO9b71Cb/fwLL8rSWdbYg9NeY6aj9LIY2+8wAdHD/DfPvFHVJWU47DbuemSq6guLeel994mFAknu4gyQTgSIRKJYLPZpqzV9XaP0Ns9wr5dLeaxytp8/vDPrqa4NBun1eCmRbl80DPM1vYhsznJZoFrynJYkuMyn7f9rUZ+9G9vmstoiJwPNR+lmaMtzXzx21/n6a2vmsdWVNfx2zferjkNKWisCelsfQrn0nysh/v/y1O893ajeaw+32s2J3ltBrdX508KhMd+voMH/uk1BYJcMIVCGgqEQvzTr/6dv3/wXxnyjwCa05Cqxjqbsz1ZGBew5WrAH+YH336Nn/+/rebIoWK3nbtrC7i7Jt9cHiPgD/Pdf3iZZx/5YPYKLwuSQiGNvbbrXf7oH/+afY2JPgXNaUg9YzUFwzDI8Z4+q3m6XttykG/91TN0tA0A4LQauEdnKHe2DfKtv3qG3du1ZIXMnDqaM4DVsPKJm+7gvt/6sPlpdCTg5/ltr9M4uiS3zA+b1UZFYTGVJeVUFpdRnFdgdv5+/h+/xtGWmV24XW47v/v5y9h0WQ0A+z9o5Qfffs2cGS1yLtPpaFYoZJB1S1aYcxrGaE7D3LIaBqUFRVQWl1FZXEZpQTHWMzQThSMR7vv6l8xd9mZqzYYKPFkOtr/VSCyqNa9lehQKC1C2N4uvfPzTXLZmg3msvaeLZ7a+St/QQBJLlhksFgsleQVUFpexqLiMisISbOfY++JoSzM7Dzfw/LbXZ1xLEJkphcICdqY5DR8cPUBXXw/dA330DPQTiUaSXMr0UJiTl6gJlJRRUVSK0+4467knOtrYebiBnYf2sfvIfvqGBuexpCLnplBY4BaXV5pzGk4Vj8cZGB6ie6CXnoF+uvt7FRajcrOyqSxJNActKirD43Kd9dzO3m52HGpg5+F97Dq8n86+nnksqcj5USgILoeDz9/5O9x66TXTOn8hhoXP4zWbgyqLy/B5vGc9t29wwKwJ7DzcQEtXxzyWVGRmFApiys/OZXF5JdUlFVSXllNdWkFVSTneaQ5dzaSwcDtdZsdwZXHZOSeWDflH2H14PzsP72PnoQaN5pK0plCQKRXl5lNdWpE2YWE1rDjsdpx2Bw67HYfNjsPuwGlPfB0/NvEcx6T7bufZm4MCoSB7jh5k5+EGdhzcx5GTTcS0o71kCIWCXLC5CotQOHzBF3OHzY7Vap3V9xmORGhoOszOQw3sPNTA/uNHiES1RIRkJoWCzLqZhkWyhCMRhgMjjAQC9A7288HRA+w81MDeY4cIhkPJLp7IvNAqqTLrOvt66OzrYfv+yWvszFVYjF3MhwN+RgJ+RgKBxNegn2F/4utIwM9wIMCIed4p5wT8hNOs30MkWRQKMiumGxZ2m00Xc5EUplCQOXW2sBCR1KRVUkVExKRQEBERk0JBRERMCgURETEpFERExKRQEBERk0JBRERMCgURETEpFERExKRQEBERk0JBRERMCgURETEpFERExKRQEBERk0JBRERMCgURETEpFERExKRQEBERk0JBRERMCgURETEpFERExKRQEBERk0JBRERMCgURETEpFERExKRQEBERk0JBRERMCgURETEpFERExKRQEBERk0JBRERMCgURETEpFERExKRQEBERk0JBRERMCgURETEpFERExKRQEBERkyUej8eTXQgREUkNqimIiIhJoSAiIiaFgoiImBQKIiJiUiiIiIhJoSAiIiaFgoiImBQKIiJiUiiIiIjp/wOZ1v0mqygdIgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "start_no = 1\n",
    "end_no = len(train_annotations_data['images'])\n",
    "# 모든 train 데이터의 이미지애 대하여..\n",
    "for img_no in range(start_no, end_no):\n",
    "    \n",
    "    annIds = train_coco.getAnnIds(imgIds=train_annotations_data['images'][img_no]['id'])\n",
    "    anns = train_coco.loadAnns(annIds)\n",
    "\n",
    "    # load and render the image\n",
    "    plt.imshow(plt.imread(TRAIN_IMAGE_DIRECTIORY+train_annotations_data['images'][img_no]['file_name']))\n",
    "    plt.axis('off')\n",
    "    # Render annotations on top of the image\n",
    "    train_coco.showAnns(anns)\n",
    "\n",
    "    mask = train_coco.annToMask(anns[0])\n",
    "    for i in range(len(anns)):\n",
    "        mask += train_coco.annToMask(anns[i])\n",
    "        plt.imshow(mask)\n",
    "        plt.axis(\"off\")\n",
    "        plt.savefig(fname='masks/' + str(train_annotations_data['images'][img_no]['id']) + '.png', bbox_inches='tight', pad_inches=0)\n",
    "    \n",
    "    # plot 초기화\n",
    "    plt.clf()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change file name\n",
    "import os\n",
    "path = \"/Users/s2hee/Desktop/deep_learning/masks\"\n",
    "file_list = os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/s2hee/Desktop/deep_learning/public_training_set_release_2.1111214.jpg/038142.png' -> '/Users/s2hee/Desktop/deep_learning/public_training_set_release_2.1111214.jpg/038142.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/s2hee/Desktop/deep_learning/basic_U_Net_.ipynb 셀 17\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/s2hee/Desktop/deep_learning/basic_U_Net_.ipynb#X22sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m src \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(path, name)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/s2hee/Desktop/deep_learning/basic_U_Net_.ipynb#X22sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m dst \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(path, newname)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/s2hee/Desktop/deep_learning/basic_U_Net_.ipynb#X22sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m os\u001b[39m.\u001b[39;49mrename(src, dst)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/s2hee/Desktop/deep_learning/public_training_set_release_2.1111214.jpg/038142.png' -> '/Users/s2hee/Desktop/deep_learning/public_training_set_release_2.1111214.jpg/038142.png'"
     ]
    }
   ],
   "source": [
    "for name in file_list:\n",
    "    newname = name.replace(\".png\", \"\")\n",
    "    newname = newname.zfill(6)\n",
    "    newname = newname + \".png\"\n",
    "    src = os.path.join(path, name)\n",
    "    dst = os.path.join(path, newname)\n",
    "    os.rename(src, dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Basic U-Net Model segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def cont_block(self, in_channels, out_channels, kernel_size=3):\n",
    "        block = nn.Sequential(nn.Conv2d(in_channels=in_channels, out_channels=out_channels, \n",
    "                                        kernel_size=kernel_size, padding=1),\n",
    "                              nn.ReLU(),\n",
    "                              nn.Dropout2d(), # p=0.5\n",
    "                              nn.Conv2d(in_channels=out_channels, out_channels=out_channels, \n",
    "                                        kernel_size=kernel_size, padding=1),\n",
    "                              nn.ReLU(),\n",
    "                              nn.BatchNorm2d(out_channels))\n",
    "        return block\n",
    "    \n",
    "    def expn_block(self, in_channels, mid_channel, out_channels, kernel_size=3):\n",
    "        block = nn.Sequential(nn.Conv2d(in_channels=in_channels, out_channels=mid_channel,\n",
    "                                        kernel_size=kernel_size, padding=1),\n",
    "                              nn.ReLU(),\n",
    "                              nn.Dropout2d(),\n",
    "                              nn.Conv2d(in_channels=mid_channel, out_channels=mid_channel,\n",
    "                                        kernel_size=kernel_size, padding=1),\n",
    "                              nn.ReLU(),\n",
    "                              nn.BatchNorm2d(mid_channel),\n",
    "                              nn.ConvTranspose2d(in_channels=mid_channel, out_channels=out_channels,\n",
    "                                                 kernel_size=kernel_size, stride=2, padding=1, output_padding=1))\n",
    "        return block\n",
    "    \n",
    "    def final_block(self, in_channels, mid_channel, out_channels, kernel_size=3):\n",
    "        block = nn.Sequential(nn.Conv2d(in_channels=in_channels, out_channels=mid_channel,\n",
    "                                        kernel_size=kernel_size, padding=1),\n",
    "                              nn.ReLU(),\n",
    "                              nn.Dropout2d(),\n",
    "                              nn.Conv2d(in_channels=mid_channel, out_channels=mid_channel,\n",
    "                                        kernel_size=kernel_size, padding=1),\n",
    "                              nn.ReLU(),\n",
    "                              nn.BatchNorm2d(mid_channel),\n",
    "                              nn.Conv2d(in_channels=mid_channel, out_channels=out_channels,\n",
    "                                        kernel_size=kernel_size, padding=1),\n",
    "                              nn.Sigmoid()\n",
    "                              )\n",
    "        return block\n",
    "    \n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super(UNet, self).__init__()\n",
    "        self.contl_1 = self.cont_block(in_channels=in_channel, out_channels=64)\n",
    "        self.contl_1_mp = nn.MaxPool2d(kernel_size=2)\n",
    "        self.contl_2 = self.cont_block(in_channels=64, out_channels=128)\n",
    "        self.contl_2_mp = nn.MaxPool2d(kernel_size=2)\n",
    "        self.contl_3 = self.cont_block(in_channels=128, out_channels=256)\n",
    "        self.contl_3_mp = nn.MaxPool2d(kernel_size=2)\n",
    "        self.contl_4 = self.cont_block(in_channels=256, out_channels=512)\n",
    "        self.contl_4_mp = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.bottleneck = nn.Sequential(nn.Conv2d(in_channels=512, out_channels=1024, \n",
    "                                                  kernel_size=3, padding=1),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.BatchNorm2d(1024),\n",
    "                                        nn.Conv2d(in_channels=1024, out_channels=1024, \n",
    "                                                  kernel_size=3, padding=1),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.BatchNorm2d(1024),\n",
    "                                        nn.ConvTranspose2d(in_channels=1024, out_channels=512,\n",
    "                                                           kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "                                        )\n",
    "        \n",
    "        self.expnl_1 = self.expn_block(1024, 512, 256)\n",
    "        self.expnl_2 = self.expn_block(512, 256, 128)\n",
    "        self.expnl_3 = self.expn_block(256, 128, 64)\n",
    "        self.final_layer = self.final_block(128, 64, out_channel)\n",
    "    \n",
    "    def crop_and_concat(self, upsampled, bypass, crop=False):\n",
    "        if crop:\n",
    "            c = (bypass.size()[2] - upsampled.size()[2]) // 2\n",
    "            bypass = F.pad(bypass, (-c, -c, -c, -c))\n",
    "        return torch.cat((upsampled, bypass), 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        contl_1_out = self.contl_1(x) \n",
    "        contl_1_pool = self.contl_1_mp(contl_1_out)\n",
    "        contl_2_out = self.contl_2(contl_1_pool)\n",
    "        contl_2_pool = self.contl_2_mp(contl_2_out)\n",
    "        contl_3_out = self.contl_3(contl_2_pool)\n",
    "        contl_3_pool = self.contl_3_mp(contl_3_out)\n",
    "        contl_4_out = self.contl_4(contl_3_pool)\n",
    "        contl_4_pool = self.contl_4_mp(contl_4_out)\n",
    "        \n",
    "        bottleneck_out = self.bottleneck(contl_4_pool)\n",
    "        \n",
    "        \"\"\" Skip connection of the expansive sub-module \"\"\"\n",
    "        expnl_1_cat = self.crop_and_concat(bottleneck_out, contl_4_out)\n",
    "        expnl_1_out = self.expnl_1(expnl_1_cat)\n",
    "        expnl_2_cat = self.crop_and_concat(expnl_1_out, contl_3_out)\n",
    "        expnl_2_out = self.expnl_2(expnl_2_cat)\n",
    "        expnl_3_cat = self.crop_and_concat(expnl_2_out, contl_2_out)\n",
    "        expnl_3_out = self.expnl_3(expnl_3_cat)\n",
    "        final_cat = self.crop_and_concat(expnl_3_out, contl_1_out)\n",
    "        final_out = self.final_layer(final_cat)\n",
    "        \n",
    "        return final_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iou definition\n",
    "def iou(pred, target, n_classes = 2):\n",
    "    \n",
    "    iou = []\n",
    "    pred = pred.view(-1)\n",
    "    target = target.view(-1)\n",
    "\n",
    "    # Ignore IoU for background class (\"0\")\n",
    "    for cls in range(1, n_classes):\n",
    "        pred_inds = pred == cls\n",
    "        target_inds = target == cls\n",
    "        intersection = (pred_inds[target_inds]).long().sum().data.cpu().item()\n",
    "        union = pred_inds.long().sum().data.cpu().item() + target_inds.long().sum().data.cpu().item() - intersection\n",
    "\n",
    "        if union == 0:\n",
    "            iou.append(float('nan'))  # If there is no ground truth, do not include in evaluation\n",
    "        else:\n",
    "            iou.append(float(intersection) / float(max(union, 1)))\n",
    "\n",
    "    return sum(iou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def iou_metric(y_pred, y_true, n_classes = 2):\n",
    "    miou = []\n",
    "    for i in np.arange(0.5, 1.0, 0.05):\n",
    "        y_pred_ = (y_pred > i)\n",
    "        iou_init = iou(y_pred_, y_true, n_classes = n_classes)\n",
    "        miou.append(iou_init)\n",
    "    \n",
    "    return sum(miou)/len(miou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basic_U_Net_.ipynb                \u001b[34mpublic_validation_set_release_2.1\u001b[m\u001b[m\n",
      "dacon_example.ipynb               \u001b[34mstage1_train\u001b[m\u001b[m\n",
      "\u001b[34mdetectron2\u001b[m\u001b[m                        x_train.npy\n",
      "\u001b[34mmasks\u001b[m\u001b[m                             y_train.npy\n",
      "\u001b[34mpublic_training_set_release_2.1\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set directory\n",
    "\n",
    "TRAIN_PATH = \"/Users/s2hee/Desktop/deep_learning/public_training_set_release_2.1\"\n",
    "VAL_PATH = \"/Users/s2hee/Desktop/deep_learning/public_validation_set_release_2.1\"\n",
    "\n",
    "train_dir = os.listdir(TRAIN_PATH)\n",
    "val_dir = os.listdir(VAL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store', 'images', 'training_annotations.json']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir_path = \"/Users/s2hee/Desktop/deep_learning/public_training_set_release_2.1/images\"\n",
    "train_ids = os.listdir(train_dir_path)\n",
    "val_dir_path = \"/Users/s2hee/Desktop/deep_learning/public_validation_set_release_2.1/images\"\n",
    "val_ids = os.listdir(val_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter\n",
    "\n",
    "IMG_WIDTH = 128\n",
    "IMG_HEIGHT = 128\n",
    "IMG_CHANNELS = 3\n",
    "width_out = 128\n",
    "height_out = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\n",
    "y_train = np.zeros((len(train_ids), height_out, width_out, 1), dtype=bool)\n",
    "x_val = np.zeros((len(val_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/s2hee/Desktop/deep_learning/basic_U_Net_.ipynb 셀 31\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/s2hee/Desktop/deep_learning/basic_U_Net_.ipynb#X42sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, id_ \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_ids):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/s2hee/Desktop/deep_learning/basic_U_Net_.ipynb#X42sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     path \u001b[39m=\u001b[39m TRAIN_PATH \u001b[39m+\u001b[39m id_\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/s2hee/Desktop/deep_learning/basic_U_Net_.ipynb#X42sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     img \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mimread(TRAIN_PATH\u001b[39m+\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m/images/\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m+\u001b[39;49mid_)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/s2hee/Desktop/deep_learning/basic_U_Net_.ipynb#X42sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     img \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mresize(img, (IMG_HEIGHT, IMG_WIDTH))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/s2hee/Desktop/deep_learning/basic_U_Net_.ipynb#X42sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     x_train[i] \u001b[39m=\u001b[39m img\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i, id_ in enumerate(train_ids):\n",
    "    path = TRAIN_PATH + id_\n",
    "    img = cv2.imread(TRAIN_PATH+'/images/'+id_)\n",
    "    img = cv2.resize(img, (IMG_HEIGHT, IMG_WIDTH))\n",
    "    x_train[i] = img\n",
    "\n",
    "    mask_path = \"/Users/s2hee/Desktop/deep_learning/masks/\"\n",
    "    mask_ = cv2.imread(mask_path+id_[:6]+\".png\", 0)\n",
    "    mask_ = cv2.resize(mask_, (height_out, width_out))\n",
    "    mask_ = np.expand_dims(mask_, axis=-1)\n",
    "\n",
    "    y_train[i] = mask_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"x_train.npy\", x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"y_train.npy\", y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation 없이 학습하기\n",
    "\n",
    "class UNetDataset(Dataset):\n",
    "    def __init__(self, images_np, masks_np):\n",
    "        self.images_np = images_np\n",
    "        self.masks_np = masks_np\n",
    "    \n",
    "    def transform(self, image_np, mask_np):\n",
    "        ToPILImage = transforms.ToPILImage()\n",
    "        image = ToPILImage(image_np)\n",
    "        mask = ToPILImage(mask_np.astype(np.int32))\n",
    "        \n",
    "        image = TF.to_tensor(image)\n",
    "        mask = TF.to_tensor(mask)\n",
    "        return image, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_np)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_np = self.images_np[idx]\n",
    "        mask_np = self.masks_np[idx]\n",
    "        image, mask = self.transform(image_np, mask_np)\n",
    "        \n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, random_state=0)\n",
    "train_dataset = UNetDataset(x_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "valid_dataset = UNetDataset(x_val, y_val)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(in_channel=3, out_channel=1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNet(\n",
      "  (contl_1): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Dropout2d(p=0.5, inplace=False)\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (contl_1_mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (contl_2): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Dropout2d(p=0.5, inplace=False)\n",
      "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (contl_2_mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (contl_3): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Dropout2d(p=0.5, inplace=False)\n",
      "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (contl_3_mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (contl_4): Sequential(\n",
      "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Dropout2d(p=0.5, inplace=False)\n",
      "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (contl_4_mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (bottleneck): Sequential(\n",
      "    (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ConvTranspose2d(1024, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  )\n",
      "  (expnl_1): Sequential(\n",
      "    (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Dropout2d(p=0.5, inplace=False)\n",
      "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  )\n",
      "  (expnl_2): Sequential(\n",
      "    (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Dropout2d(p=0.5, inplace=False)\n",
      "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  )\n",
      "  (expnl_3): Sequential(\n",
      "    (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Dropout2d(p=0.5, inplace=False)\n",
      "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  )\n",
      "  (final_layer): Sequential(\n",
      "    (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Dropout2d(p=0.5, inplace=False)\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "alpha = 5\n",
    "batch_size = 16\n",
    "# nn.CrossEntropyLoss() (paper) -> BCELoss()\n",
    "criterion=nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "train_iou_sum = 0\n",
    "valid_iou_sum = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1  Training Loss:  tensor(0.0155, device='mps:0', grad_fn=<DivBackward0>) Validation Loss:  tensor(3.1915e-05, device='mps:0')\n",
      "Training IoU:  0.9799779721428373 Validation IoU:  1.0\n",
      "Epoch  2  Training Loss:  tensor(1.2031e-05, device='mps:0', grad_fn=<DivBackward0>) Validation Loss:  tensor(4.8718e-06, device='mps:0')\n",
      "Training IoU:  0.9999999953625248 Validation IoU:  1.0\n",
      "Epoch  3  Training Loss:  tensor(2.2070e-06, device='mps:0', grad_fn=<DivBackward0>) Validation Loss:  tensor(1.0599e-06, device='mps:0')\n",
      "Training IoU:  0.9999999958113128 Validation IoU:  1.0\n"
     ]
    }
   ],
   "source": [
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "train_iou_list = []\n",
    "val_iou_list = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_iou = 0\n",
    "\n",
    "    for image, mask in train_loader:\n",
    "        image = image.to(device)\n",
    "        mask = mask.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(image.float())\n",
    "    \n",
    "        loss = criterion(outputs.float(), mask.float())\n",
    "        train_loss += loss\n",
    "\n",
    "        train_iou += iou_metric(outputs, mask)\n",
    "        rev_iou = 16 - iou_metric(outputs, mask)\n",
    "        loss += alpha * rev_iou\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = 0\n",
    "        valid_iou = 0\n",
    "\n",
    "        for image_val, mask_val in valid_loader:\n",
    "            image_val = image_val.to(device)\n",
    "            mask_val = mask_val.to(device)\n",
    "            output_val = model(image_val.float())\n",
    "            valid_loss += criterion(output_val.float(), mask_val.float())\n",
    "            valid_iou += iou_metric(output_val, mask_val)\n",
    "\n",
    "    print(\"Epoch \", epoch + 1, \" Training Loss: \", train_loss/len(train_loader), \"Validation Loss: \", valid_loss/len(valid_loader))\n",
    "    print(\"Training IoU: \", train_iou/len(train_loader), \"Validation IoU: \", valid_iou/len(valid_loader))\n",
    "    train_iou_sum += train_iou/len(train_loader)\n",
    "    valid_iou_sum += valid_iou/len(valid_loader)\n",
    "\n",
    "    # visualization\n",
    "    train_loss_list.append(train_loss/len(train_loader))\n",
    "    val_loss_list.append(valid_loss/len(valid_loader))\n",
    "    train_iou_list.append(train_iou/len(train_loader))\n",
    "    val_iou_list.append(valid_iou/len(valid_loader)\n",
    ")\n",
    "    \n",
    "print(\"Training Mean IoU: {:.2f}\".format(train_iou_sum/epochs), \" Validation Mean IoU: {:.2f}\".format(valid_iou_sum/epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss, IOU 값 시각화하기\n",
    "\n",
    "train_loss_data = [i.detach().cpu().numpy() for i in train_loss_list]\n",
    "val_loss_data = [i.detach().cpu().numpy() for i in val_loss_list]\n",
    "# train_iou_data = [i.detach().cpu().numpy() for i in train_iou_list]\n",
    "# val_iou_data = [i.detach().cpu().numpy() for i in val_iou_list]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 6)) \n",
    "plt.subplot(1,2,1)\n",
    "plt.title('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.plot(train_loss_data, 'b', label='train loss')\n",
    "plt.plot(val_loss_data, 'g', label='val loss')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('iou')\n",
    "plt.xlabel('epoch')\n",
    "plt.plot(train_iou_list, 'b', label='train iou')\n",
    "plt.plot(val_iou_list, 'g', label='val iou')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제값, 예측값 시각화하기\n",
    "\n",
    "for img_, mask_ in valid_loader:\n",
    "    img = img_[1].to(device)\n",
    "    img.unsqueeze_(0)\n",
    "    mask_pred = model(img.float())\n",
    "    mask_pred = mask_pred.cpu()\n",
    "    mask_pred = (mask_pred > 0.75)\n",
    "    mask_true = mask_[1]\n",
    "\n",
    "    img = TF.to_pil_image(mask_pred.float().squeeze(0))\n",
    "    mask = TF.to_pil_image(mask_true)\n",
    "\n",
    "    img = np.array(img)\n",
    "    mask = np.array(mask)\n",
    "\n",
    "    fig, (axis_1, axis_2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    axis_1.imshow(img.astype(np.uint8), cmap='gray')\n",
    "    axis_1.set_title('Input Image')\n",
    "    axis_2.imshow(mask.astype(np.uint8), cmap='gray')\n",
    "    axis_2.set_title('Prediction')\n",
    "    plt.show()\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation 적용하여 학습하기\n",
    "\n",
    "class UNetDataset_DAug(Dataset):\n",
    "    def __init__(self, images_np, masks_np):\n",
    "        self.images_np = images_np\n",
    "        self.masks_np = masks_np\n",
    "    \n",
    "    def transform(self, image_np, mask_np):\n",
    "        ToPILImage = transforms.ToPILImage()\n",
    "        image = ToPILImage(image_np)\n",
    "        mask = ToPILImage(mask_np.astype(np.int32))\n",
    "        \n",
    "        image = TF.pad(image, padding = 20, padding_mode = 'reflect')\n",
    "        mask = TF.pad(mask, padding = 20, padding_mode = 'reflect')\n",
    "              \n",
    "        angle = random.uniform(-10, 10)\n",
    "        width, height = image.size\n",
    "        max_dx = 0.1 * width\n",
    "        max_dy = 0.1 * height\n",
    "        translations = (np.round(random.uniform(-max_dx, max_dx)), np.round(random.uniform(-max_dy, max_dy)))\n",
    "        scale = random.uniform(0.8, 1.2)\n",
    "        shear = random.uniform(-0.5, 0.5)\n",
    "        image = TF.affine(image, angle = angle, translate = translations, scale = scale, shear = shear)\n",
    "        mask = TF.affine(mask, angle = angle, translate = translations, scale = scale, shear = shear)\n",
    "        \n",
    "\n",
    "        image = TF.center_crop(image, (128, 128))\n",
    "        mask = TF.center_crop(mask, (128, 128))\n",
    "        \n",
    "        image = TF.to_tensor(image)\n",
    "        mask = TF.to_tensor(mask)\n",
    "        return image, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_np)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_np = self.images_np[idx]\n",
    "        mask_np = self.masks_np[idx]\n",
    "        image, mask = self.transform(image_np, mask_np)\n",
    "        \n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, random_state=0)\n",
    "train_dataset = UNetDataset_DAug(x_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "valid_dataset = UNetDataset_DAug(x_val, y_val)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = UNet(in_channel=3, out_channel=1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 25 \n",
    "alpha = 5\n",
    "batch_size = 16 \n",
    "# nn.CrossEntropyLoss() (paper) -> BCELoss()\n",
    "criterion=nn.BCELoss()\n",
    "optimizer = optim.Adam(model2.parameters(), lr=1e-3)\n",
    "train_iou_sum = 0\n",
    "valid_iou_sum = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "train_iou_list = []\n",
    "val_iou_list = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model2.train()\n",
    "    train_loss = 0\n",
    "    train_iou = 0\n",
    "\n",
    "    for image, mask in train_loader:\n",
    "        image = image.to(device)\n",
    "        mask = mask.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model2(image.float())\n",
    "    \n",
    "        loss = criterion(outputs.float(), mask.float())\n",
    "        train_loss += loss\n",
    "\n",
    "        train_iou += iou_metric(outputs, mask)\n",
    "        rev_iou = 16 - iou_metric(outputs, mask)\n",
    "        loss += alpha * rev_iou\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model2.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = 0\n",
    "        valid_iou = 0\n",
    "\n",
    "        for image_val, mask_val in valid_loader:\n",
    "            image_val = image_val.to(device)\n",
    "            mask_val = mask_val.to(device)\n",
    "            output_val = model2(image_val.float())\n",
    "            valid_loss += criterion(output_val.float(), mask_val.float())\n",
    "            valid_iou += iou_metric(output_val, mask_val)\n",
    "\n",
    "    print(\"Epoch \", epoch + 1, \" Training Loss: \", train_loss/len(train_loader), \"Validation Loss: \", valid_loss/len(valid_loader))\n",
    "    print(\"Training IoU: \", train_iou/len(train_loader), \"Validation IoU: \", valid_iou/len(valid_loader))\n",
    "    train_iou_sum += train_iou/len(train_loader)\n",
    "    valid_iou_sum += valid_iou/len(valid_loader)\n",
    "\n",
    "    # visualization\n",
    "    train_loss_list.append(train_loss/len(train_loader))\n",
    "    val_loss_list.append(valid_loss/len(valid_loader))\n",
    "    train_iou_list.append(train_iou/len(train_loader))\n",
    "    val_iou_list.append(valid_iou/len(valid_loader)\n",
    ")\n",
    "    \n",
    "print(\"Training Mean IoU: {:.2f}\".format(train_iou_sum/epochs), \" Validation Mean IoU: {:.2f}\".format(valid_iou_sum/epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss, IOU 값 시각화\n",
    "\n",
    "train_loss_data = [i.detach().cpu().numpy() for i in train_loss_list]\n",
    "val_loss_data = [i.detach().cpu().numpy() for i in val_loss_list]\n",
    "# train_iou_data = [i.detach().cpu().numpy() for i in train_iou_list]\n",
    "# val_iou_data = [i.detach().cpu().numpy() for i in val_iou_list]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 6)) \n",
    "plt.subplot(1,2,1)\n",
    "plt.title('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.plot(train_loss_data, 'b', label='train loss')\n",
    "plt.plot(val_loss_data, 'g', label='val loss')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('iou')\n",
    "plt.xlabel('epoch')\n",
    "plt.plot(train_iou_list, 'b', label='train iou')\n",
    "plt.plot(val_iou_list, 'g', label='val iou')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 실제값, 예측값 시각화\n",
    "\n",
    "for img_, mask_ in valid_loader:\n",
    "    img = img_[1].to(device)\n",
    "    img.unsqueeze_(0)\n",
    "    mask_pred = model2(img.float())\n",
    "    mask_pred = mask_pred.cpu()\n",
    "    mask_pred = (mask_pred > 0.75)\n",
    "    mask_true = mask_[1]\n",
    "\n",
    "    img = TF.to_pil_image(mask_pred.float().squeeze(0))\n",
    "    mask = TF.to_pil_image(mask_true)\n",
    "\n",
    "    img = np.array(img)\n",
    "    mask = np.array(mask)\n",
    "\n",
    "    fig, (axis_1, axis_2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    axis_1.imshow(img.astype(np.uint8), cmap='gray')\n",
    "    axis_1.set_title('Input Image')\n",
    "    axis_2.imshow(mask.astype(np.uint8), cmap='gray')\n",
    "    axis_2.set_title('Prediction')\n",
    "    plt.show()\n",
    "\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('sehee')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "87b29be79caa31f39437f92f2a865b23c2f4e0693e3bfc42c7f510d610300d74"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
